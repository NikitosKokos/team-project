{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted for 1_user5.mp4\n",
      "Frames extracted for 1_user12.mp4\n",
      "Frames extracted for 1_user6.mp4\n",
      "Frames extracted for 1_user13.mp4\n",
      "Frames extracted for 0_user2.mp4\n",
      "Frames extracted for 0_user3.mp4\n",
      "Frames extracted for 0_user19.mp4\n",
      "Frames extracted for 0_user20.mp4\n",
      "Frames extracted for 0_user10.mp4\n",
      "Frames extracted for 1_user22.mp4\n",
      "Frames extracted for 1_user23.mp4\n",
      "Frames extracted for 0.5_user1.mp4\n",
      "Frames extracted for 1_user21.mp4\n",
      "Frames extracted for 1_user8.mp4\n",
      "Frames extracted for 1_user9.mp4\n",
      "Frame extraction completed!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "video_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/videos\"\n",
    "frames_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/frames\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "def extract_frames(video_path, output_dir, interval=1):\n",
    "    \"\"\"\n",
    "    Extract frames from a video at a given interval (default: every 1 frame).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    frame_count = 0\n",
    "    video_name = os.path.basename(video_path).rsplit('.', 1)[0]  # Get the video name without extension\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save every 'interval' frame\n",
    "        if count % interval == 0:\n",
    "            frame_path = os.path.join(output_dir, f\"{video_name}_frame{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Process all videos\n",
    "for video_file in os.listdir(video_dir):\n",
    "    # Check if the filename ends with \".mp4\" and starts with a valid number\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        try:\n",
    "            # Split by '_' and attempt to convert the first part to float\n",
    "            prefix = video_file.split('_')[0]\n",
    "            float(prefix)  # This ensures it works for values like '0.5', '1', or '0'\n",
    "            \n",
    "            # Proceed with frame extraction\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            video_frames_dir = os.path.join(frames_dir, video_file.rsplit('.', 1)[0])  # Use full name\n",
    "            os.makedirs(video_frames_dir, exist_ok=True)\n",
    "            extract_frames(video_path, video_frames_dir)\n",
    "            print(f\"Frames extracted for {video_file}\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping file with invalid prefix: {video_file}\")\n",
    "\n",
    "print(\"Frame extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737121652.167359 6038652 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1737121652.221196 6039441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737121652.253168 6039446 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737121652.302850 6039444 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0.5_user1\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0_user3\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0_user2\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0_user20\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user13\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user5\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user22\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user23\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0_user10\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user12\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 0_user19\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user8\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user21\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user6\n",
      "Keypoints, pelvis rotation, and javelin retraction detection completed for 1_user9\n",
      "Processing completed!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Paths\n",
    "frames_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/frames\"\n",
    "keypoints_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/keypoints\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(keypoints_dir, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, enable_segmentation=False)\n",
    "\n",
    "def extract_keypoints_from_frame(frame_path):\n",
    "    \"\"\"\n",
    "    Extract keypoints relevant to the javelin throwing motion.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(frame_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(rgb_image)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        keypoints = {\n",
    "            \"right_hand\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].z,\n",
    "            },\n",
    "            \"right_shoulder\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].z,\n",
    "            },\n",
    "            \"left_hand\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].z,\n",
    "            },\n",
    "            \"left_shoulder\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].z,\n",
    "            },\n",
    "            \"pelvis_left\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].z,\n",
    "            },\n",
    "            \"pelvis_right\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].z,\n",
    "            },\n",
    "            \"torso\": {\n",
    "                \"x\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x,\n",
    "                \"y\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y,\n",
    "                \"z\": result.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].z,\n",
    "            },\n",
    "        }\n",
    "        return keypoints\n",
    "    return None\n",
    "\n",
    "def check_pelvis_rotation(keypoints_sequence):\n",
    "    \"\"\"\n",
    "    Check if the pelvis is rotated during the throwing motion (analyzing left and right hip position).\n",
    "    \"\"\"\n",
    "    rotated_positions = []\n",
    "\n",
    "    # Analyze pelvis rotation in the last 5 frames\n",
    "    for keypoints in keypoints_sequence[-5:]:\n",
    "        pelvis_left = keypoints[\"pelvis_left\"]\n",
    "        pelvis_right = keypoints[\"pelvis_right\"]\n",
    "\n",
    "        # Check if the left and right hips have shifted (x-coordinate comparison)\n",
    "        if pelvis_left[\"x\"] > pelvis_right[\"x\"]:\n",
    "            rotated_positions.append(True)\n",
    "        else:\n",
    "            rotated_positions.append(False)\n",
    "\n",
    "    # Verify if the pelvis is consistently rotated in the last 5 frames\n",
    "    return all(rotated_positions)\n",
    "\n",
    "def check_javelin_fully_retracted(keypoints_sequence):\n",
    "    \"\"\"\n",
    "    Check if the javelin is fully retracted during the last 5 steps.\n",
    "    \"\"\"\n",
    "    fully_retracted = []\n",
    "\n",
    "    # Analyze the last 5 frames\n",
    "    for keypoints in keypoints_sequence[-5:]:\n",
    "        right_hand = keypoints[\"right_hand\"]\n",
    "        right_shoulder = keypoints[\"right_shoulder\"]\n",
    "\n",
    "        # Check if the right hand is behind the right shoulder (x-coordinate comparison)\n",
    "        if right_hand[\"x\"] < right_shoulder[\"x\"]:\n",
    "            fully_retracted.append(True)\n",
    "        else:\n",
    "            fully_retracted.append(False)\n",
    "\n",
    "    # Verify if the javelin is consistently retracted in the last 5 frames\n",
    "    return all(fully_retracted)\n",
    "\n",
    "def process_frames(video_frames_dir, output_path):\n",
    "    \"\"\"\n",
    "    Process all frames for a video, detect pelvis rotation and javelin retraction, and save keypoints as JSON.\n",
    "    \"\"\"\n",
    "    keypoints_data = []\n",
    "    for frame_file in sorted(os.listdir(video_frames_dir)):\n",
    "        frame_path = os.path.join(video_frames_dir, frame_file)\n",
    "        keypoints = extract_keypoints_from_frame(frame_path)\n",
    "        if keypoints:\n",
    "            keypoints_data.append(keypoints)\n",
    "\n",
    "    # Check for pelvis rotation and javelin retraction\n",
    "    pelvis_rotation_detected = check_pelvis_rotation(keypoints_data)\n",
    "    javelin_fully_retracted = check_javelin_fully_retracted(keypoints_data)\n",
    "\n",
    "    # Save keypoints and detection status to JSON\n",
    "    output_data = {\n",
    "        \"keypoints\": keypoints_data,\n",
    "        \"pelvis_rotation_detected\": pelvis_rotation_detected,\n",
    "        \"javelin_fully_retracted\": javelin_fully_retracted,\n",
    "    }\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "# Process frames for each video\n",
    "for video_name in os.listdir(frames_dir):\n",
    "    video_frames_dir = os.path.join(frames_dir, video_name)\n",
    "    if os.path.isdir(video_frames_dir):\n",
    "        output_path = os.path.join(keypoints_dir, f\"{video_name}_keypoints.json\")\n",
    "        process_frames(video_frames_dir, output_path)\n",
    "        print(f\"Keypoints, pelvis rotation, and javelin retraction detection completed for {video_name}\")\n",
    "\n",
    "print(\"Processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP features extracted for 0.5_user1\n",
      "CLIP features extracted for 0_user3\n",
      "CLIP features extracted for 0_user2\n",
      "CLIP features extracted for 0_user20\n",
      "CLIP features extracted for 1_user13\n",
      "CLIP features extracted for 1_user5\n",
      "CLIP features extracted for 1_user22\n",
      "CLIP features extracted for 1_user23\n",
      "CLIP features extracted for 0_user10\n",
      "CLIP features extracted for 1_user12\n",
      "CLIP features extracted for 0_user19\n",
      "CLIP features extracted for 1_user8\n",
      "CLIP features extracted for 1_user21\n",
      "CLIP features extracted for 1_user6\n",
      "CLIP features extracted for 1_user9\n",
      "CLIP feature extraction completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "frames_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/frames\"\n",
    "clip_features_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/clip_features\"\n",
    "os.makedirs(clip_features_dir, exist_ok=True)\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def extract_clip_features(video_frames_dir, output_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extract CLIP features for all frames in a video using batch processing.\n",
    "    \"\"\"\n",
    "    frame_features = []\n",
    "    frame_paths = sorted(os.listdir(video_frames_dir))\n",
    "    images = []\n",
    "\n",
    "    for i, frame_file in enumerate(frame_paths):\n",
    "        try:\n",
    "            frame_path = os.path.join(video_frames_dir, frame_file)\n",
    "            image = Image.open(frame_path).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "\n",
    "            # Process batch\n",
    "            if len(images) == batch_size or i == len(frame_paths) - 1:\n",
    "                inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    image_features = model.get_image_features(**inputs).cpu().numpy()\n",
    "                    frame_features.extend(image_features)\n",
    "                images = []  # Clear batch to free memory\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save features\n",
    "    torch.save(frame_features, output_path)\n",
    "\n",
    "# Process all videos\n",
    "for video_name in os.listdir(frames_dir):\n",
    "    video_frames_dir = os.path.join(frames_dir, video_name)\n",
    "    if os.path.isdir(video_frames_dir):\n",
    "        output_path = os.path.join(clip_features_dir, f\"{video_name}_clip.pt\")\n",
    "        extract_clip_features(video_frames_dir, output_path)\n",
    "        print(f\"CLIP features extracted for {video_name}\")\n",
    "\n",
    "print(\"CLIP feature extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0_user3\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user8\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user23\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user9\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user22\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user6\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0_user2\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0_user19\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0_user10\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user21\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user5\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user12\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 1_user13\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0_user20\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "KeyError: Missing key 'right_hip' in the keypoints data\n",
      "Combined features saved for 0.5_user1\n",
      "Feature combination completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/3038619333.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clip_data = torch.load(clip_path)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Paths\n",
    "keypoints_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/keypoints\"\n",
    "clip_features_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/clip_features\"\n",
    "combined_features_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/combined_features\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(combined_features_dir, exist_ok=True)\n",
    "\n",
    "# Function to calculate angle between three points (a, b, c) where b is the vertex\n",
    "def calculate_angle(a, b, c):\n",
    "    ba = [a[0] - b[0], a[1] - b[1]]\n",
    "    bc = [c[0] - b[0], c[1] - b[1]]\n",
    "    \n",
    "    dot_product = ba[0] * bc[0] + ba[1] * bc[1]\n",
    "    magnitude_a = math.sqrt(ba[0] ** 2 + ba[1] ** 2)\n",
    "    magnitude_b = math.sqrt(bc[0] ** 2 + bc[1] ** 2)\n",
    "    \n",
    "    cos_angle = dot_product / (magnitude_a * magnitude_b)\n",
    "    angle = math.acos(cos_angle) * (180.0 / math.pi)\n",
    "    \n",
    "    return angle\n",
    "\n",
    "# Detect pelvis rotation and javelin fully retracted position by analyzing keypoints\n",
    "def detect_pelvis_rotation_and_javelin_retraction(keypoints_sequence):\n",
    "    pelvis_rotation_detected = False\n",
    "    javelin_retracted_detected = False\n",
    "\n",
    "    for i in range(len(keypoints_sequence) - 5, len(keypoints_sequence)):\n",
    "        try:\n",
    "            # Focus on pelvis (hip) and arm positions for detecting rotation and retraction\n",
    "            right_hip = keypoints_sequence[i][\"right_shoulder\"]  # Substitute with actual hip landmark if needed\n",
    "            left_hip = keypoints_sequence[i][\"left_shoulder\"]  # Substitute with actual hip landmark if needed\n",
    "            right_hand = keypoints_sequence[i][\"right_hand\"]\n",
    "            left_hand = keypoints_sequence[i][\"left_hand\"]\n",
    "\n",
    "            # Pelvis rotation: check if the pelvis has rotated by a certain angle (e.g., 30 degrees or more)\n",
    "            pelvis_angle = calculate_angle(right_hip, left_hip, keypoints_sequence[i - 1][\"right_hip\"])\n",
    "            if pelvis_angle > 30:  # Adjust this threshold based on the exact body motion\n",
    "                pelvis_rotation_detected = True\n",
    "\n",
    "            # Javelin retraction: check if the right hand is behind the right shoulder (indicating retraction)\n",
    "            if right_hand[\"x\"] < right_hip[\"x\"]:  # Check if the hand is behind the hip in the x-axis\n",
    "                javelin_retracted_detected = True\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: Missing key {e} in the keypoints data\")\n",
    "\n",
    "    return pelvis_rotation_detected, javelin_retracted_detected\n",
    "\n",
    "# Combine pose-based features and CLIP embeddings for each video\n",
    "def combine_features(video_name, pose_path, clip_path, output_path):\n",
    "    with open(pose_path, 'r') as f:\n",
    "        keypoints_data = json.load(f)[\"keypoints\"]\n",
    "\n",
    "    # Detect pelvis rotation and javelin retraction during the last steps\n",
    "    pelvis_rotation, javelin_retracted = detect_pelvis_rotation_and_javelin_retraction(keypoints_data)\n",
    "\n",
    "    # Load CLIP embeddings\n",
    "    clip_data = torch.load(clip_path)\n",
    "\n",
    "    # Combine features for each frame\n",
    "    combined_data = []\n",
    "    for i, clip_frame in enumerate(clip_data):\n",
    "        pose_features = {\n",
    "            \"pelvis_rotation\": 1 if pelvis_rotation else 0,  # Binary flag for pelvis rotation\n",
    "            \"javelin_retracted\": 1 if javelin_retracted else 0,  # Binary flag for javelin retraction\n",
    "        }\n",
    "        combined_frame = np.concatenate([clip_frame, list(pose_features.values())])\n",
    "        combined_data.append(combined_frame)\n",
    "\n",
    "    # Save combined features\n",
    "    torch.save(combined_data, output_path)\n",
    "\n",
    "# Process all videos and combine features\n",
    "for video_name in os.listdir(keypoints_dir):\n",
    "    if video_name.endswith(\"_keypoints.json\"):\n",
    "        video_name_base = video_name.replace(\"_keypoints.json\", \"\")\n",
    "        pose_path = os.path.join(keypoints_dir, video_name)\n",
    "        clip_path = os.path.join(clip_features_dir, f\"{video_name_base}_clip.pt\")\n",
    "        output_path = os.path.join(combined_features_dir, f\"{video_name_base}_combined.pt\")\n",
    "\n",
    "        if os.path.exists(clip_path):\n",
    "            combine_features(video_name_base, pose_path, clip_path, output_path)\n",
    "            print(f\"Combined features saved for {video_name_base}\")\n",
    "\n",
    "print(\"Feature combination completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created. Input size: 515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/3694048949.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  video_features = torch.tensor(torch.load(os.path.join(combined_features_dir, file)), dtype=torch.float32)\n",
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/3694048949.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  video_features = torch.tensor(torch.load(os.path.join(combined_features_dir, file)), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "combined_features_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/combined_features\"\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 20  # Adjust as needed (still remains as sequence length)\n",
    "batch_size = 16       # Adjust as needed\n",
    "\n",
    "# Ensure input size is always 515\n",
    "input_size = 515\n",
    "\n",
    "class AthleticsDataset(Dataset):\n",
    "    def __init__(self, combined_features_dir, sequence_length, input_size):\n",
    "        \"\"\"\n",
    "        Handles loading and processing of combined features for athletics data.\n",
    "        Ensures input size is always 515.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_size = input_size\n",
    "\n",
    "        for file in os.listdir(combined_features_dir):\n",
    "            if file.endswith(\"_combined.pt\"):\n",
    "                # Load combined features\n",
    "                video_features = torch.tensor(torch.load(os.path.join(combined_features_dir, file)), dtype=torch.float32)\n",
    "\n",
    "                # Truncate or pad sequences to the desired length\n",
    "                if video_features.shape[0] >= self.sequence_length:\n",
    "                    video_features = video_features[:self.sequence_length]\n",
    "                else:\n",
    "                    padding = torch.zeros((self.sequence_length - video_features.shape[0], video_features.shape[1]))\n",
    "                    video_features = torch.cat((video_features, padding), dim=0)\n",
    "\n",
    "                # Ensure features match the input size (515)\n",
    "                if video_features.shape[1] != self.input_size:\n",
    "                    padding = torch.zeros((video_features.shape[0], self.input_size - video_features.shape[1]))\n",
    "                    video_features = torch.cat((video_features, padding), dim=1)\n",
    "                self.data.append(video_features)\n",
    "\n",
    "                # Extract label from filename\n",
    "                label = float(file.split(\"_\")[0])  # Extract label from file name\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "def get_data_loaders(combined_features_dir, sequence_length, batch_size, input_size, train_split=0.8):\n",
    "    dataset = AthleticsDataset(combined_features_dir, sequence_length, input_size)\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, input_size  # Return loaders and input size\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader, input_size = get_data_loaders(combined_features_dir, sequence_length, batch_size, input_size)\n",
    "\n",
    "print(f\"DataLoaders created. Input size: {input_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        LSTM-based model for sequence prediction.\n",
    "        \"\"\"\n",
    "        super(TemporalModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)  # Use the final hidden state\n",
    "        output = self.fc(hidden[-1])  # Fully connected output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "combined_features_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/combined_features\"\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class AthleticsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, combined_features_dir, sequence_length):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        for file in os.listdir(combined_features_dir):\n",
    "            if file.endswith(\"_combined.pt\"):\n",
    "                video_features = torch.tensor(torch.load(os.path.join(combined_features_dir, file)), dtype=torch.float32)\n",
    "\n",
    "                if video_features.shape[0] >= self.sequence_length:\n",
    "                    video_features = video_features[:self.sequence_length]\n",
    "                else:\n",
    "                    padding = torch.zeros((self.sequence_length - video_features.shape[0], video_features.shape[1]))\n",
    "                    video_features = torch.cat((video_features, padding), dim=0)\n",
    "\n",
    "                self.data.append(video_features)\n",
    "                label = float(file.split(\"_\")[0])  # Extract label from file name\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def get_data_loaders(combined_features_dir, sequence_length, batch_size, train_split=0.8):\n",
    "    dataset = AthleticsDataset(combined_features_dir, sequence_length)\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, dataset[0][0].shape[1]\n",
    "\n",
    "# Define LSTM model\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(TemporalModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=500, patience=100):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # # Early stopping logic with 100 epochs patience\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "\n",
    "        # if patience_counter >= patience:\n",
    "        #     print(f\"Early stopping triggered after {patience} epochs without improvement!\")\n",
    "        #     break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# Hyperparameter grid search\n",
    "# def hyperparameter_search():\n",
    "    # Hyperparameter grid\n",
    "    hidden_sizes = [32, 64, 128]\n",
    "    num_layers = [1, 2, 3]\n",
    "    learning_rates = [0.001, 0.005, 0.0001, 0.0005]\n",
    "    dropouts = [0.0, 0.2, 0.3]\n",
    "\n",
    "    # Initialize variables to track the best model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_params = None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Prepare data\n",
    "    train_loader, val_loader, input_size = get_data_loaders(combined_features_dir, sequence_length=20, batch_size=8)\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for hidden_size, num_layer, learning_rate, dropout in itertools.product(hidden_sizes, num_layers, learning_rates, dropouts):\n",
    "        print(f\"Testing configuration: Hidden Size={hidden_size}, Num Layers={num_layer}, LR={learning_rate}, Dropout={dropout}\")\n",
    "        \n",
    "        # Initialize model, criterion, and optimizer\n",
    "        model = TemporalModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layer, output_size=1, dropout=dropout).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        val_loss = train_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Update best model if this configuration is better\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = {\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"num_layers\": num_layer,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"dropout\": dropout,\n",
    "            }\n",
    "\n",
    "    print(f\"Best Configuration: {best_params}, Validation Loss: {best_val_loss:.4f}\")\n",
    "    return best_params\n",
    "\n",
    "# Run hyperparameter search\n",
    "# best_params = hyperparameter_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/1542848075.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  video_features = torch.tensor(torch.load(os.path.join(combined_features_dir, file)), dtype=torch.float32)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/1542848075.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features, labels = features.to(device), torch.tensor(labels, dtype=torch.float32).to(device)\n",
      "/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_94727/1542848075.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features, labels = features.to(device), torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size for the model: 514\n",
      "Epoch [1/1000], Train Loss: 0.6913, Val Loss: 0.1854\n",
      "Epoch [2/1000], Train Loss: 0.2938, Val Loss: 0.3344\n",
      "Epoch [3/1000], Train Loss: 0.2538, Val Loss: 0.4546\n",
      "Epoch [4/1000], Train Loss: 0.2950, Val Loss: 0.4143\n",
      "Epoch [5/1000], Train Loss: 0.2543, Val Loss: 0.3243\n",
      "Epoch [6/1000], Train Loss: 0.1980, Val Loss: 0.2489\n",
      "Epoch [7/1000], Train Loss: 0.1651, Val Loss: 0.2032\n",
      "Epoch [8/1000], Train Loss: 0.1564, Val Loss: 0.1806\n",
      "Epoch [9/1000], Train Loss: 0.1586, Val Loss: 0.1714\n",
      "Epoch [10/1000], Train Loss: 0.1601, Val Loss: 0.1696\n",
      "Epoch [11/1000], Train Loss: 0.1554, Val Loss: 0.1732\n",
      "Epoch [12/1000], Train Loss: 0.1442, Val Loss: 0.1824\n",
      "Epoch [13/1000], Train Loss: 0.1287, Val Loss: 0.1983\n",
      "Epoch [14/1000], Train Loss: 0.1123, Val Loss: 0.2232\n",
      "Epoch [15/1000], Train Loss: 0.0981, Val Loss: 0.2590\n",
      "Epoch [16/1000], Train Loss: 0.0881, Val Loss: 0.3042\n",
      "Epoch [17/1000], Train Loss: 0.0822, Val Loss: 0.3517\n",
      "Epoch [18/1000], Train Loss: 0.0784, Val Loss: 0.3901\n",
      "Epoch [19/1000], Train Loss: 0.0734, Val Loss: 0.4087\n",
      "Epoch [20/1000], Train Loss: 0.0652, Val Loss: 0.3982\n",
      "Epoch [21/1000], Train Loss: 0.0538, Val Loss: 0.3539\n",
      "Epoch [22/1000], Train Loss: 0.0413, Val Loss: 0.2945\n",
      "Epoch [23/1000], Train Loss: 0.0300, Val Loss: 0.2456\n",
      "Epoch [24/1000], Train Loss: 0.0214, Val Loss: 0.2113\n",
      "Epoch [25/1000], Train Loss: 0.0159, Val Loss: 0.1954\n",
      "Epoch [26/1000], Train Loss: 0.0128, Val Loss: 0.2067\n",
      "Epoch [27/1000], Train Loss: 0.0112, Val Loss: 0.2490\n",
      "Epoch [28/1000], Train Loss: 0.0106, Val Loss: 0.3030\n",
      "Epoch [29/1000], Train Loss: 0.0111, Val Loss: 0.3515\n",
      "Epoch [30/1000], Train Loss: 0.0118, Val Loss: 0.3873\n",
      "Epoch [31/1000], Train Loss: 0.0118, Val Loss: 0.4117\n",
      "Epoch [32/1000], Train Loss: 0.0106, Val Loss: 0.4238\n",
      "Epoch [33/1000], Train Loss: 0.0084, Val Loss: 0.4237\n",
      "Epoch [34/1000], Train Loss: 0.0058, Val Loss: 0.4148\n",
      "Epoch [35/1000], Train Loss: 0.0035, Val Loss: 0.3997\n",
      "Epoch [36/1000], Train Loss: 0.0017, Val Loss: 0.3779\n",
      "Epoch [37/1000], Train Loss: 0.0010, Val Loss: 0.3557\n",
      "Epoch [38/1000], Train Loss: 0.0011, Val Loss: 0.3436\n",
      "Epoch [39/1000], Train Loss: 0.0015, Val Loss: 0.3403\n",
      "Epoch [40/1000], Train Loss: 0.0018, Val Loss: 0.3388\n",
      "Epoch [41/1000], Train Loss: 0.0020, Val Loss: 0.3366\n",
      "Epoch [42/1000], Train Loss: 0.0022, Val Loss: 0.3342\n",
      "Epoch [43/1000], Train Loss: 0.0022, Val Loss: 0.3311\n",
      "Epoch [44/1000], Train Loss: 0.0021, Val Loss: 0.3267\n",
      "Epoch [45/1000], Train Loss: 0.0019, Val Loss: 0.3209\n",
      "Epoch [46/1000], Train Loss: 0.0016, Val Loss: 0.3139\n",
      "Epoch [47/1000], Train Loss: 0.0013, Val Loss: 0.3069\n",
      "Epoch [48/1000], Train Loss: 0.0011, Val Loss: 0.3008\n",
      "Epoch [49/1000], Train Loss: 0.0009, Val Loss: 0.2963\n",
      "Epoch [50/1000], Train Loss: 0.0008, Val Loss: 0.2936\n",
      "Epoch [51/1000], Train Loss: 0.0006, Val Loss: 0.2928\n",
      "Epoch [52/1000], Train Loss: 0.0006, Val Loss: 0.2935\n",
      "Epoch [53/1000], Train Loss: 0.0005, Val Loss: 0.2955\n",
      "Epoch [54/1000], Train Loss: 0.0005, Val Loss: 0.2988\n",
      "Epoch [55/1000], Train Loss: 0.0005, Val Loss: 0.3031\n",
      "Epoch [56/1000], Train Loss: 0.0004, Val Loss: 0.3081\n",
      "Epoch [57/1000], Train Loss: 0.0004, Val Loss: 0.3135\n",
      "Epoch [58/1000], Train Loss: 0.0003, Val Loss: 0.3186\n",
      "Epoch [59/1000], Train Loss: 0.0003, Val Loss: 0.3231\n",
      "Epoch [60/1000], Train Loss: 0.0003, Val Loss: 0.3266\n",
      "Epoch [61/1000], Train Loss: 0.0002, Val Loss: 0.3293\n",
      "Epoch [62/1000], Train Loss: 0.0002, Val Loss: 0.3312\n",
      "Epoch [63/1000], Train Loss: 0.0002, Val Loss: 0.3327\n",
      "Epoch [64/1000], Train Loss: 0.0002, Val Loss: 0.3339\n",
      "Epoch [65/1000], Train Loss: 0.0001, Val Loss: 0.3350\n",
      "Epoch [66/1000], Train Loss: 0.0001, Val Loss: 0.3360\n",
      "Epoch [67/1000], Train Loss: 0.0001, Val Loss: 0.3366\n",
      "Epoch [68/1000], Train Loss: 0.0001, Val Loss: 0.3367\n",
      "Epoch [69/1000], Train Loss: 0.0001, Val Loss: 0.3362\n",
      "Epoch [70/1000], Train Loss: 0.0001, Val Loss: 0.3350\n",
      "Epoch [71/1000], Train Loss: 0.0001, Val Loss: 0.3331\n",
      "Epoch [72/1000], Train Loss: 0.0001, Val Loss: 0.3308\n",
      "Epoch [73/1000], Train Loss: 0.0001, Val Loss: 0.3283\n",
      "Epoch [74/1000], Train Loss: 0.0001, Val Loss: 0.3259\n",
      "Epoch [75/1000], Train Loss: 0.0001, Val Loss: 0.3240\n",
      "Epoch [76/1000], Train Loss: 0.0001, Val Loss: 0.3228\n",
      "Epoch [77/1000], Train Loss: 0.0001, Val Loss: 0.3224\n",
      "Epoch [78/1000], Train Loss: 0.0001, Val Loss: 0.3229\n",
      "Epoch [79/1000], Train Loss: 0.0001, Val Loss: 0.3241\n",
      "Epoch [80/1000], Train Loss: 0.0000, Val Loss: 0.3258\n",
      "Epoch [81/1000], Train Loss: 0.0000, Val Loss: 0.3278\n",
      "Epoch [82/1000], Train Loss: 0.0000, Val Loss: 0.3297\n",
      "Epoch [83/1000], Train Loss: 0.0000, Val Loss: 0.3313\n",
      "Epoch [84/1000], Train Loss: 0.0000, Val Loss: 0.3322\n",
      "Epoch [85/1000], Train Loss: 0.0000, Val Loss: 0.3325\n",
      "Epoch [86/1000], Train Loss: 0.0000, Val Loss: 0.3322\n",
      "Epoch [87/1000], Train Loss: 0.0000, Val Loss: 0.3315\n",
      "Epoch [88/1000], Train Loss: 0.0000, Val Loss: 0.3306\n",
      "Epoch [89/1000], Train Loss: 0.0000, Val Loss: 0.3298\n",
      "Epoch [90/1000], Train Loss: 0.0000, Val Loss: 0.3292\n",
      "Epoch [91/1000], Train Loss: 0.0000, Val Loss: 0.3287\n",
      "Epoch [92/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [93/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [94/1000], Train Loss: 0.0000, Val Loss: 0.3279\n",
      "Epoch [95/1000], Train Loss: 0.0000, Val Loss: 0.3275\n",
      "Epoch [96/1000], Train Loss: 0.0000, Val Loss: 0.3271\n",
      "Epoch [97/1000], Train Loss: 0.0000, Val Loss: 0.3266\n",
      "Epoch [98/1000], Train Loss: 0.0000, Val Loss: 0.3261\n",
      "Epoch [99/1000], Train Loss: 0.0000, Val Loss: 0.3255\n",
      "Epoch [100/1000], Train Loss: 0.0000, Val Loss: 0.3251\n",
      "Epoch [101/1000], Train Loss: 0.0000, Val Loss: 0.3249\n",
      "Epoch [102/1000], Train Loss: 0.0000, Val Loss: 0.3249\n",
      "Epoch [103/1000], Train Loss: 0.0000, Val Loss: 0.3251\n",
      "Epoch [104/1000], Train Loss: 0.0000, Val Loss: 0.3256\n",
      "Epoch [105/1000], Train Loss: 0.0000, Val Loss: 0.3263\n",
      "Epoch [106/1000], Train Loss: 0.0000, Val Loss: 0.3271\n",
      "Epoch [107/1000], Train Loss: 0.0000, Val Loss: 0.3278\n",
      "Epoch [108/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [109/1000], Train Loss: 0.0000, Val Loss: 0.3290\n",
      "Epoch [110/1000], Train Loss: 0.0000, Val Loss: 0.3293\n",
      "Epoch [111/1000], Train Loss: 0.0000, Val Loss: 0.3295\n",
      "Epoch [112/1000], Train Loss: 0.0000, Val Loss: 0.3296\n",
      "Epoch [113/1000], Train Loss: 0.0000, Val Loss: 0.3296\n",
      "Epoch [114/1000], Train Loss: 0.0000, Val Loss: 0.3296\n",
      "Epoch [115/1000], Train Loss: 0.0000, Val Loss: 0.3296\n",
      "Epoch [116/1000], Train Loss: 0.0000, Val Loss: 0.3296\n",
      "Epoch [117/1000], Train Loss: 0.0000, Val Loss: 0.3297\n",
      "Epoch [118/1000], Train Loss: 0.0000, Val Loss: 0.3298\n",
      "Epoch [119/1000], Train Loss: 0.0000, Val Loss: 0.3299\n",
      "Epoch [120/1000], Train Loss: 0.0000, Val Loss: 0.3299\n",
      "Epoch [121/1000], Train Loss: 0.0000, Val Loss: 0.3298\n",
      "Epoch [122/1000], Train Loss: 0.0000, Val Loss: 0.3297\n",
      "Epoch [123/1000], Train Loss: 0.0000, Val Loss: 0.3294\n",
      "Epoch [124/1000], Train Loss: 0.0000, Val Loss: 0.3292\n",
      "Epoch [125/1000], Train Loss: 0.0000, Val Loss: 0.3289\n",
      "Epoch [126/1000], Train Loss: 0.0000, Val Loss: 0.3287\n",
      "Epoch [127/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [128/1000], Train Loss: 0.0000, Val Loss: 0.3284\n",
      "Epoch [129/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [130/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [131/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [132/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [133/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [134/1000], Train Loss: 0.0000, Val Loss: 0.3282\n",
      "Epoch [135/1000], Train Loss: 0.0000, Val Loss: 0.3282\n",
      "Epoch [136/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [137/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [138/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [139/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [140/1000], Train Loss: 0.0000, Val Loss: 0.3281\n",
      "Epoch [141/1000], Train Loss: 0.0000, Val Loss: 0.3282\n",
      "Epoch [142/1000], Train Loss: 0.0000, Val Loss: 0.3283\n",
      "Epoch [143/1000], Train Loss: 0.0000, Val Loss: 0.3284\n",
      "Epoch [144/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [145/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [146/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [147/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [148/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [149/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [150/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [151/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [152/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [153/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [154/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [155/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [156/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [157/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [158/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [159/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [160/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [161/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [162/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [163/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [164/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [165/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [166/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [167/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [168/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [169/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [170/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [171/1000], Train Loss: 0.0000, Val Loss: 0.3286\n",
      "Epoch [172/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [173/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [174/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [175/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [176/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [177/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [178/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [179/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [180/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [181/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [182/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [183/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [184/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [185/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [186/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [187/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [188/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [189/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [190/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [191/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [192/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [193/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [194/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [195/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [196/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [197/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [198/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [199/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [200/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [201/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [202/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [203/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [204/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [205/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [206/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [207/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [208/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [209/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [210/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [211/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [212/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [213/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [214/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [215/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [216/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [217/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [218/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [219/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [220/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [221/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [222/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [223/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [224/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [225/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [226/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [227/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [228/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [229/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [230/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [231/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [232/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [233/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [234/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [235/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [236/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [237/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [238/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [239/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [240/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [241/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [242/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [243/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [244/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [245/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [246/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [247/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [248/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [249/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [250/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [251/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [252/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [253/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [254/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [255/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [256/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [257/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [258/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [259/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [260/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [261/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [262/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [263/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [264/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [265/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [266/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [267/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [268/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [269/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [270/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [271/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [272/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [273/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [274/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [275/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [276/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [277/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [278/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [279/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [280/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [281/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [282/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [283/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [284/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [285/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [286/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [287/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [288/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [289/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [290/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [291/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [292/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [293/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [294/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [295/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [296/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [297/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [298/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [299/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [300/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [301/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [302/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [303/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [304/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [305/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [306/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [307/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [308/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [309/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [310/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [311/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [312/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [313/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [314/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [315/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [316/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [317/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [318/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [319/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [320/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [321/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [322/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [323/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [324/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [325/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [326/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [327/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [328/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [329/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [330/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [331/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [332/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [333/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [334/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [335/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [336/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [337/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [338/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [339/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [340/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [341/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [342/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [343/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [344/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [345/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [346/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [347/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [348/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [349/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [350/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [351/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [352/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [353/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [354/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [355/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [356/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [357/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [358/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [359/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [360/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [361/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [362/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [363/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [364/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [365/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [366/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [367/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [368/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [369/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [370/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [371/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [372/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [373/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [374/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [375/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [376/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [377/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [378/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [379/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [380/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [381/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [382/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [383/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [384/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [385/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [386/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [387/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [388/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [389/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [390/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [391/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [392/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [393/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [394/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [395/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [396/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [397/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [398/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [399/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [400/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [401/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [402/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [403/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [404/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [405/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [406/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [407/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [408/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [409/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [410/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [411/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [412/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [413/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [414/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [415/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [416/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [417/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [418/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [419/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [420/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [421/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [422/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [423/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [424/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [425/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [426/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [427/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [428/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [429/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [430/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [431/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [432/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [433/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [434/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [435/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [436/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [437/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [438/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [439/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [440/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [441/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [442/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [443/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [444/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [445/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [446/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [447/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [448/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [449/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [450/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [451/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [452/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [453/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [454/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [455/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [456/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [457/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [458/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [459/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [460/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [461/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [462/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [463/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [464/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [465/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [466/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [467/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [468/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [469/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [470/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [471/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [472/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [473/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [474/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [475/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [476/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [477/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [478/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [479/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [480/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [481/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [482/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [483/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [484/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [485/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [486/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [487/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [488/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [489/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [490/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [491/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [492/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [493/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [494/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [495/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [496/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [497/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [498/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [499/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [500/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [501/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [502/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [503/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [504/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [505/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [506/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [507/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [508/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [509/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [510/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [511/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [512/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [513/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [514/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [515/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [516/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [517/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [518/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [519/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [520/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [521/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [522/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [523/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [524/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [525/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [526/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [527/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [528/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [529/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [530/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [531/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [532/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [533/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [534/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [535/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [536/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [537/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [538/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [539/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [540/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [541/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [542/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [543/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [544/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [545/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [546/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [547/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [548/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [549/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [550/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [551/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [552/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [553/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [554/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [555/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [556/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [557/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [558/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [559/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [560/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [561/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [562/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [563/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [564/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [565/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [566/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [567/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [568/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [569/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [570/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [571/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [572/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [573/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [574/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [575/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [576/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [577/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [578/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [579/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [580/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [581/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [582/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [583/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [584/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [585/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [586/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [587/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [588/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [589/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [590/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [591/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [592/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [593/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [594/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [595/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [596/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [597/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [598/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [599/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [600/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [601/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [602/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [603/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [604/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [605/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [606/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [607/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [608/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [609/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [610/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [611/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [612/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [613/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [614/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [615/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [616/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [617/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [618/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [619/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [620/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [621/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [622/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [623/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [624/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [625/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [626/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [627/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [628/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [629/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [630/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [631/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [632/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [633/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [634/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [635/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [636/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [637/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [638/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [639/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [640/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [641/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [642/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [643/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [644/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [645/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [646/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [647/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [648/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [649/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [650/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [651/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [652/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [653/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [654/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [655/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [656/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [657/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [658/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [659/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [660/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [661/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [662/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [663/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [664/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [665/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [666/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [667/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [668/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [669/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [670/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [671/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [672/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [673/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [674/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [675/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [676/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [677/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [678/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [679/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [680/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [681/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [682/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [683/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [684/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [685/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [686/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [687/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [688/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [689/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [690/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [691/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [692/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [693/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [694/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [695/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [696/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [697/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [698/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [699/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [700/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [701/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [702/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [703/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [704/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [705/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [706/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [707/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [708/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [709/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [710/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [711/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [712/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [713/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [714/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [715/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [716/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [717/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [718/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [719/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [720/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [721/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [722/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [723/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [724/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [725/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [726/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [727/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [728/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [729/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [730/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [731/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [732/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [733/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [734/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [735/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [736/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [737/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [738/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [739/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [740/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [741/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [742/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [743/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [744/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [745/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [746/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [747/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [748/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [749/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [750/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [751/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [752/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [753/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [754/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [755/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [756/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [757/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [758/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [759/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [760/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [761/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [762/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [763/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [764/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [765/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [766/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [767/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [768/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [769/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [770/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [771/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [772/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [773/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [774/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [775/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [776/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [777/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [778/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [779/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [780/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [781/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [782/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [783/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [784/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [785/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [786/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [787/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [788/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [789/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [790/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [791/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [792/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [793/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [794/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [795/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [796/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [797/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [798/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [799/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [800/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [801/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [802/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [803/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [804/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [805/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [806/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [807/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [808/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [809/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [810/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [811/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [812/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [813/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [814/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [815/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [816/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [817/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [818/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [819/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [820/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [821/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [822/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [823/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [824/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [825/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [826/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [827/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [828/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [829/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [830/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [831/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [832/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [833/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [834/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [835/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [836/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [837/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [838/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [839/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [840/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [841/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [842/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [843/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [844/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [845/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [846/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [847/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [848/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [849/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [850/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [851/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [852/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [853/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [854/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [855/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [856/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [857/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [858/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [859/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [860/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [861/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [862/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [863/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [864/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [865/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [866/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [867/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [868/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [869/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [870/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [871/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [872/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [873/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [874/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [875/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [876/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [877/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [878/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [879/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [880/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [881/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [882/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [883/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [884/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [885/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [886/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [887/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [888/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [889/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [890/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [891/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [892/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [893/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [894/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [895/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [896/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [897/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [898/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [899/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [900/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [901/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [902/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [903/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [904/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [905/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [906/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [907/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [908/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [909/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [910/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [911/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [912/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [913/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [914/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [915/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [916/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [917/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [918/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [919/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [920/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [921/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [922/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [923/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [924/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [925/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [926/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [927/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [928/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [929/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [930/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [931/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [932/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [933/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [934/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [935/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [936/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [937/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [938/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [939/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [940/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [941/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [942/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [943/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [944/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [945/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [946/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [947/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [948/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [949/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [950/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [951/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [952/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [953/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [954/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [955/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [956/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [957/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [958/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [959/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [960/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [961/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [962/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [963/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [964/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [965/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [966/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [967/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [968/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [969/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [970/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [971/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [972/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [973/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [974/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [975/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [976/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [977/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [978/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [979/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [980/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [981/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [982/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [983/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [984/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [985/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [986/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [987/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [988/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [989/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [990/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [991/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [992/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [993/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [994/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [995/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [996/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [997/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [998/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [999/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Epoch [1000/1000], Train Loss: 0.0000, Val Loss: 0.3285\n",
      "Final model trained. Validation Loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Train the final model with the best configuration\n",
    "final_hidden_size = 128\n",
    "final_num_layers = 1\n",
    "final_learning_rate = 0.0012\n",
    "final_dropout = 0.5\n",
    "\n",
    "# Prepare DataLoaders (use full dataset for training)\n",
    "train_loader, val_loader, input_size = get_data_loaders(combined_features_dir, sequence_length=20, batch_size=32)\n",
    "print(f\"Input size for the model: {input_size}\")\n",
    "\n",
    "# Initialize final model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "final_model = TemporalModel(input_size=input_size, hidden_size=final_hidden_size, num_layers=final_num_layers, output_size=1, dropout=final_dropout).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=final_learning_rate)\n",
    "\n",
    "# Train final model\n",
    "final_val_loss = train_model(final_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=1000, patience=100)\n",
    "print(f\"Final model trained. Validation Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_collect_predictions(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided DataLoader and collect true vs. predicted values.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(features).squeeze()\n",
    "            predicted_labels.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "# Use validation set for evaluation\n",
    "true_labels, predicted_labels = evaluate_and_collect_predictions(final_model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737121772.707069 6038652 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Pro\n",
      "W0000 00:00:1737121772.766003 6045024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737121772.805769 6045024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 1_user5.mp4\n",
      "Prediction: 0.8274\n",
      "Processing video: 1_user12.mp4\n",
      "Prediction: 0.8670\n",
      "Processing video: 1_user6.mp4\n",
      "Prediction: 0.9539\n",
      "Processing video: 1_user13.mp4\n",
      "Prediction: 0.6803\n",
      "Processing video: 0_user2.mp4\n",
      "Prediction: 0.6615\n",
      "Processing video: 0_user3.mp4\n",
      "Prediction: 0.8220\n",
      "Processing video: 0_user19.mp4\n",
      "Prediction: 0.5842\n",
      "Processing video: 0_user20.mp4\n",
      "Prediction: 0.6684\n",
      "Processing video: 0_user10.mp4\n",
      "Prediction: 0.6120\n",
      "Processing video: 1_user22.mp4\n",
      "Prediction: 0.9202\n",
      "Processing video: 1_user23.mp4\n",
      "Prediction: 0.7648\n",
      "Processing video: 0.5_user1.mp4\n",
      "Prediction: 0.8721\n",
      "Processing video: 1_user21.mp4\n",
      "Prediction: 0.8991\n",
      "Processing video: 1_user8.mp4\n",
      "Prediction: 0.9425\n",
      "Processing video: 1_user9.mp4\n",
      "Prediction: 0.9611\n",
      "\n",
      "Prediction Results:\n",
      "1_user5.mp4: 0.8274\n",
      "1_user12.mp4: 0.8670\n",
      "1_user6.mp4: 0.9539\n",
      "1_user13.mp4: 0.6803\n",
      "0_user2.mp4: 0.6615\n",
      "0_user3.mp4: 0.8220\n",
      "0_user19.mp4: 0.5842\n",
      "0_user20.mp4: 0.6684\n",
      "0_user10.mp4: 0.6120\n",
      "1_user22.mp4: 0.9202\n",
      "1_user23.mp4: 0.7648\n",
      "0.5_user1.mp4: 0.8721\n",
      "1_user21.mp4: 0.8991\n",
      "1_user8.mp4: 0.9425\n",
      "1_user9.mp4: 0.9611\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import mediapipe as mp\n",
    "\n",
    "# Paths for javelin throwing\n",
    "video_dir = \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/videos\"\n",
    "sequence_length = 20  # Sequence length for LSTM\n",
    "\n",
    "# Initialize CLIP and MediaPipe\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, enable_segmentation=False)\n",
    "\n",
    "# Ensure input size is always 515 for javelin throwing\n",
    "input_size = 514\n",
    "\n",
    "# Load your model (final_model should be defined elsewhere)\n",
    "# final_model = ...  # Make sure to load your trained model here\n",
    "\n",
    "# Process and Predict for each video\n",
    "def process_and_predict(video_path, model, sequence_length):\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # Step 1: Extract frames from video\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % 5 == 0:  # Extract frame every 5th frame\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(rgb_frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Step 2: Extract pose keypoints using MediaPipe\n",
    "    keypoints = []\n",
    "    for frame in frames:\n",
    "        result = pose.process(frame)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints.append([\n",
    "                {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z, \"visibility\": lm.visibility}\n",
    "                for lm in result.pose_landmarks.landmark\n",
    "            ])\n",
    "    \n",
    "    # Step 3: Detect large stride before the throw (run-up phase)\n",
    "    run_up_stride_scores = []\n",
    "    for i in range(1, len(keypoints)):\n",
    "        if i < len(keypoints):  # Check bounds\n",
    "            lead_leg_prev = keypoints[i - 1][25]  # Right knee (Lead leg)\n",
    "            lead_leg_curr = keypoints[i][25]  # Right knee (Lead leg)\n",
    "\n",
    "            # Calculate stride length (distance between two consecutive positions of lead leg)\n",
    "            stride_length = np.sqrt((lead_leg_curr['x'] - lead_leg_prev['x'])**2 + (lead_leg_curr['y'] - lead_leg_prev['y'])**2)\n",
    "\n",
    "            # Define a large stride threshold based on experimentation or domain knowledge\n",
    "            large_stride_threshold = 0.05  # Adjust this threshold based on your requirements\n",
    "            run_up_stride_scores.append(stride_length > large_stride_threshold)\n",
    "\n",
    "    # Step 4: Analyze shoulder, elbow, and wrist alignment during the throw (post-throw mechanics)\n",
    "    throw_alignment_scores = []\n",
    "    for i in range(len(keypoints)):\n",
    "        if i < len(keypoints):  # Check bounds\n",
    "            shoulder = keypoints[i][11]  # Left shoulder (Torso)\n",
    "            elbow = keypoints[i][13]    # Left elbow\n",
    "            wrist = keypoints[i][15]    # Left wrist\n",
    "\n",
    "            # Vectors for shoulder -> elbow and elbow -> wrist\n",
    "            shoulder_to_elbow = np.array([elbow['x'] - shoulder['x'], elbow['y'] - shoulder['y']])\n",
    "            elbow_to_wrist = np.array([wrist['x'] - elbow['x'], wrist['y'] - elbow['y']])\n",
    "\n",
    "            # Calculate the angle between shoulder-elbow and elbow-wrist (angle of throw)\n",
    "            angle = np.arctan2(elbow_to_wrist[1], elbow_to_wrist[0]) - np.arctan2(shoulder_to_elbow[1], shoulder_to_elbow[0])\n",
    "            throw_alignment_scores.append(np.degrees(angle))\n",
    "\n",
    "    # Step 5: Detect backward motion during the last 5 frames (Javelin brought backward)\n",
    "    backward_motion_scores = []\n",
    "    hand_positions = [keypoints[i][15] for i in range(len(keypoints))]  # Track the wrist (dominant hand)\n",
    "\n",
    "    for i in range(len(hand_positions) - 5, len(hand_positions)):\n",
    "        if i > 0:  # Ensure we are not accessing out of bounds\n",
    "            prev_position = hand_positions[i - 1]\n",
    "            curr_position = hand_positions[i]\n",
    "\n",
    "            # Calculate displacement in X, Y (backward motion detection)\n",
    "            displacement = np.sqrt((curr_position['x'] - prev_position['x'])**2 + (curr_position['y'] - prev_position['y'])**2)\n",
    "            # Determine if the hand moved backward (negative movement along x-axis)\n",
    "            backward_motion_scores.append(curr_position['x'] < prev_position['x'])\n",
    "\n",
    "    # Step 6: Extract CLIP embeddings for frames (semantic features)\n",
    "    clip_embeddings = []\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(frame)\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.get_image_features(**inputs).cpu().numpy().flatten()\n",
    "            clip_embeddings.append(embedding)\n",
    "\n",
    "    # Step 7: Combine all features (CLIP, stride, throw alignment, backward motion)\n",
    "    combined_features = []\n",
    "    for i, clip_embedding in enumerate(clip_embeddings):\n",
    "        feature_vector = np.concatenate([clip_embedding, \n",
    "                                        [run_up_stride_scores[i] if i < len(run_up_stride_scores) else 0],\n",
    "                                        [throw_alignment_scores[i] if i < len(throw_alignment_scores) else 0],\n",
    "                                        [backward_motion_scores[i] if i < len(backward_motion_scores) else 0]])\n",
    "        \n",
    "        if len(feature_vector) > input_size:\n",
    "            feature_vector = feature_vector[:input_size]  # Truncate if too long\n",
    "        elif len(feature_vector) < input_size:\n",
    "            padding = np.zeros(input_size - len(feature_vector))  # Pad with zeros if too short\n",
    "            feature_vector = np.concatenate([feature_vector, padding])\n",
    "        combined_features.append(feature_vector)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    combined_features = torch.tensor(combined_features, dtype=torch.float32)\n",
    "\n",
    "    # Step 8: Truncate or pad sequences to the fixed sequence length\n",
    "    if combined_features.shape[0] >= sequence_length:\n",
    "        combined_features = combined_features[:sequence_length]\n",
    "    else:\n",
    "        padding = torch.zeros((sequence_length - combined_features.shape[0], combined_features.shape[1]))\n",
    "        combined_features = torch.cat((combined_features, padding), dim=0)\n",
    "\n",
    "    # Step 9: Predict using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        combined_features = combined_features.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        prediction = model(combined_features).squeeze().cpu().item()\n",
    "\n",
    "    print(f\"Prediction: {prediction:.4f}\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Process all videos in the directory and predict\n",
    "results = {}\n",
    "for video_file in os.listdir(video_dir):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        prediction = process_and_predict(video_path, final_model, sequence_length)\n",
    "        results[video_file] = prediction\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPrediction Results:\")\n",
    "for video, prediction in results.items():\n",
    "    print(f\"{video}: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"/Users/danyukezz/Desktop/2 year 1 semester/team project/danya_preprocessing_sports/spear_throwing/stages/stage2/stage2_javelin.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feedback generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_metadata(video_path, model, sequence_length):\n",
    "    \"\"\"\n",
    "    Process a video and return structured metadata with predictions and pose metrics.\n",
    "    \"\"\"\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    frames = extract_frames(video_path)\n",
    "    \n",
    "    # Step 2: Extract keypoints\n",
    "    keypoints = extract_keypoints(frames)\n",
    "\n",
    "    # Step 3: Calculate pose-based features\n",
    "    velocities, accelerations = calculate_velocity_and_acceleration(keypoints)\n",
    "    stride_lengths = calculate_stride_length(keypoints)\n",
    "\n",
    "    # Pose metrics\n",
    "    pose_metrics = {\n",
    "        \"average_velocity\": np.mean(velocities) if velocities else 0,\n",
    "        \"max_stride_length\": np.max(stride_lengths) if stride_lengths else 0,\n",
    "        \"average_acceleration\": np.mean(accelerations) if accelerations else 0\n",
    "    }\n",
    "\n",
    "    # Step 4: Extract CLIP features\n",
    "    clip_embeddings = extract_clip_features(frames)\n",
    "\n",
    "    # CLIP insights\n",
    "    clip_summary = \"High contextual alignment with accelerating motion.\"  # Placeholder for now\n",
    "\n",
    "    # Step 5: Combine features\n",
    "    combined_features = combine_features(clip_embeddings, velocities, accelerations, stride_lengths)\n",
    "\n",
    "    # Step 6: Truncate or pad sequences\n",
    "    if combined_features.shape[0] >= sequence_length:\n",
    "        combined_features = combined_features[:sequence_length]\n",
    "    else:\n",
    "        padding = torch.zeros((sequence_length - combined_features.shape[0], combined_features.shape[1]))\n",
    "        combined_features = torch.cat((combined_features, padding), dim=0)\n",
    "\n",
    "    # Step 7: Predict using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        combined_features = combined_features.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        prediction = model(combined_features).squeeze().cpu().item()\n",
    "\n",
    "    print(f\"Prediction: {prediction:.4f}\")\n",
    "\n",
    "    # Return structured metadata\n",
    "    return {\n",
    "        \"video_name\": os.path.basename(video_path),\n",
    "        \"prediction\": prediction,\n",
    "        \"pose_metrics\": pose_metrics,\n",
    "        \"clip_features\": {\"embedding_summary\": clip_summary}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "# result = generator(\"Explain the performance of an athlete based on metrics.\", max_length=50)\n",
    "# print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "# def validate_metadata(metadata):\n",
    "#     required_keys = {\n",
    "#         'video_name': str,\n",
    "#         'prediction': float,\n",
    "#         'pose_metrics': dict,\n",
    "#         'clip_features': dict,\n",
    "#     }\n",
    "#     pose_metrics_keys = ['average_velocity', 'max_stride_length', 'average_acceleration']\n",
    "#     clip_features_keys = ['embedding_summary']\n",
    "\n",
    "#     for key, expected_type in required_keys.items():\n",
    "#         if key not in metadata or not isinstance(metadata[key], expected_type):\n",
    "#             raise ValueError(f\"Invalid or missing key: {key}, expected type: {expected_type}\")\n",
    "    \n",
    "#     for key in pose_metrics_keys:\n",
    "#         if key not in metadata['pose_metrics']:\n",
    "#             raise ValueError(f\"Missing pose metric: {key}\")\n",
    "    \n",
    "#     for key in clip_features_keys:\n",
    "#         if key not in metadata['clip_features']:\n",
    "#             raise ValueError(f\"Missing clip feature: {key}\")\n",
    "\n",
    "# def generate_justification(metadata, max_length=150, num_return_sequences=1):\n",
    "#     validate_metadata(metadata)\n",
    "    \n",
    "#     # Dynamic prompt construction\n",
    "#     metrics_prompt = []\n",
    "#     for metric, value in metadata['pose_metrics'].items():\n",
    "#         metrics_prompt.append(f\"- {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "#     metrics_text = \"\\n\".join(metrics_prompt)\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     Analyze the performance for {metadata['video_name']}.\n",
    "#     The predicted score is {metadata['prediction']:.2f}.\n",
    "#     Key metrics:\n",
    "#     {metrics_text}\n",
    "#     - CLIP embedding summary: {metadata['clip_features']['embedding_summary']}\n",
    "    \n",
    "#     Based on these metrics, explain why the score is appropriate and provide constructive feedback for improvement.\n",
    "#     \"\"\"\n",
    "#     print(\"Generated prompt:\", prompt)  # Debugging log\n",
    "#     result = generator(prompt, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "#     return result[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory containing videos\n",
    "# video_dir = \"/Users/cezar/Desktop/Team Project/AI/distance_jump/stage1/videos\"\n",
    "\n",
    "# # Dictionary to store results\n",
    "# results = {}\n",
    "\n",
    "# for video_file in os.listdir(video_dir):\n",
    "#     if video_file.endswith(\".mp4\"):\n",
    "#         video_path = os.path.join(video_dir, video_file)\n",
    "        \n",
    "#         # Step 1: Process video and generate metadata\n",
    "#         metadata = process_video_with_metadata(video_path, final_model, sequence_length=20)\n",
    "        \n",
    "#         # Step 2: Generate justification using Hugging Face model\n",
    "#         justification = generate_justification(metadata)\n",
    "#         metadata[\"justification\"] = justification\n",
    "        \n",
    "#         # Store results\n",
    "#         results[video_file] = metadata\n",
    "\n",
    "# # Print results\n",
    "# for video, data in results.items():\n",
    "#     print(f\"Video: {video}\")\n",
    "#     print(f\"Prediction: {data['prediction']:.4f}\")\n",
    "#     print(f\"Justification: {data['justification']}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Optional: Save results to JSON\n",
    "# # import json\n",
    "# # with open(\"video_predictions_with_justifications.json\", \"w\") as f:\n",
    "# #     json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
