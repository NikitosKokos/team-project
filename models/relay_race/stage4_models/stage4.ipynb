{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated video saved to: stage4-interpolated\\user1.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user10.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user12.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user13.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user19.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user2.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user20.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user22.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user23.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user3.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user5.mp4\n",
      "Interpolated video saved to: stage4-interpolated\\user8.mp4\n",
      "Interpolation complete for all videos.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_folder = \"stage4\"\n",
    "output_folder = \"stage4-interpolated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def interpolate_frames(frame1, frame2, num_interpolations=3):\n",
    "    \"\"\"\n",
    "    Interpolates between two frames using linear interpolation.\n",
    "    \n",
    "    Args:\n",
    "        frame1 (numpy.ndarray): First frame.\n",
    "        frame2 (numpy.ndarray): Second frame.\n",
    "        num_interpolations (int): Number of interpolated frames to generate.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of interpolated frames.\n",
    "    \"\"\"\n",
    "    interpolated_frames = []\n",
    "    for i in range(1, num_interpolations + 1):\n",
    "        alpha = i / (num_interpolations + 1)  # Interpolation ratio\n",
    "        interpolated_frame = cv2.addWeighted(frame1, 1 - alpha, frame2, alpha, 0)\n",
    "        interpolated_frames.append(interpolated_frame)\n",
    "    return interpolated_frames\n",
    "\n",
    "def process_video(video_path, output_path, num_interpolations=3):\n",
    "    \"\"\"\n",
    "    Processes a video to add interpolated frames and saves the result.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the interpolated video.\n",
    "        num_interpolations (int): Number of interpolated frames between each pair of original frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "    \n",
    "    # New FPS after interpolation\n",
    "    new_fps = fps * (num_interpolations + 1)\n",
    "    out = cv2.VideoWriter(output_path, fourcc, new_fps, (width, height))\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    while ret:\n",
    "        ret, next_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Write the original frame\n",
    "        out.write(prev_frame)\n",
    "\n",
    "        # Generate and write interpolated frames\n",
    "        interpolated_frames = interpolate_frames(prev_frame, next_frame, num_interpolations)\n",
    "        for frame in interpolated_frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        # Update the previous frame\n",
    "        prev_frame = next_frame\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Interpolated video saved to: {output_path}\")\n",
    "\n",
    "# Process all videos in the input folder\n",
    "num_interpolations = 3  # Number of interpolated frames between each pair\n",
    "\n",
    "for video_file in os.listdir(input_folder):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, video_file)\n",
    "        process_video(input_path, output_path, num_interpolations=num_interpolations)\n",
    "\n",
    "print(\"Interpolation complete for all videos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average frames per video: 219.33\n",
      "Minimum frames in a video: 120\n",
      "Maximum frames in a video: 356\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the videos\n",
    "stage_path = \"stage4-interpolated\"\n",
    "\n",
    "# List to store the number of frames for each video\n",
    "frame_counts = []\n",
    "\n",
    "# Iterate through the videos in the folder\n",
    "for file in os.listdir(stage_path):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        video_file_path = os.path.join(stage_path, file)\n",
    "        \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_file_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {file}\")\n",
    "            continue\n",
    "\n",
    "        # Get the total number of frames\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_counts.append(total_frames)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "# Calculate average, minimum, and maximum frame counts\n",
    "if frame_counts:\n",
    "    avg_frames = sum(frame_counts) / len(frame_counts)\n",
    "    min_frames = min(frame_counts)\n",
    "    max_frames = max(frame_counts)\n",
    "\n",
    "    print(f\"Average frames per video: {avg_frames:.2f}\")\n",
    "    print(f\"Minimum frames in a video: {min_frames}\")\n",
    "    print(f\"Maximum frames in a video: {max_frames}\")\n",
    "else:\n",
    "    print(\"No valid videos found in the specified folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: stage4-interpolated\\user1.mp4 -> stage4-200frames\\user1.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user10.mp4 -> stage4-200frames\\user10.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user12.mp4 -> stage4-200frames\\user12.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user13.mp4 -> stage4-200frames\\user13.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user19.mp4 -> stage4-200frames\\user19.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user2.mp4 -> stage4-200frames\\user2.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user20.mp4 -> stage4-200frames\\user20.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user22.mp4 -> stage4-200frames\\user22.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user23.mp4 -> stage4-200frames\\user23.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user3.mp4 -> stage4-200frames\\user3.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user5.mp4 -> stage4-200frames\\user5.mp4 with 200 frames.\n",
      "Processed: stage4-interpolated\\user8.mp4 -> stage4-200frames\\user8.mp4 with 200 frames.\n",
      "All videos resized to exactly 200 frames using truncate or pad method.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to input and output directories\n",
    "input_dir = \"stage4-interpolated\"\n",
    "output_dir = \"stage4-200frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "target_frames = 200  # Target number of frames per video\n",
    "\n",
    "def truncate_or_pad_video(video_path, output_path, target_frames):\n",
    "    \"\"\"\n",
    "    Truncate or pad a video to ensure it has exactly `target_frames` frames.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the processed video.\n",
    "        target_frames (int): Desired number of frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create VideoWriter for the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < target_frames:\n",
    "        # Pad by duplicating the last frame until reaching the target\n",
    "        while len(frames) < target_frames:\n",
    "            frames.append(frames[-1])  # Duplicate the last frame\n",
    "    elif len(frames) > target_frames:\n",
    "        # Truncate the video to the target number of frames\n",
    "        frames = frames[:target_frames]\n",
    "\n",
    "    # Write the processed frames to the output video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    print(f\"Processed: {video_path} -> {output_path} with {target_frames} frames.\")\n",
    "\n",
    "# Process all videos in the input directory\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(input_dir, file)\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "        truncate_or_pad_video(input_path, output_path, target_frames)\n",
    "\n",
    "print(\"All videos resized to exactly 200 frames using truncate or pad method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented videos saved to stage4-dataset and labels saved to stage4-dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_folder = \"stage4-200frames\"\n",
    "output_folder = \"stage4-dataset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"stage4.csv\")  # Ensure this CSV contains 'video' and 'label' columns\n",
    "\n",
    "# Augmentation Functions\n",
    "def augment_video(video_path, output_path, augmentation_type, is_odd):\n",
    "    \"\"\"\n",
    "    Augment the video with the specified augmentation type and save it.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the augmented video.\n",
    "        augmentation_type (str): Type of augmentation ('rotate', 'brightness', 'noise', etc.).\n",
    "        is_odd (bool): Whether the video index is odd or even.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "\n",
    "    # Create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply augmentation\n",
    "        if augmentation_type == \"mirrored\":\n",
    "            frame = cv2.flip(frame, 1)  # Horizontal flip\n",
    "        elif augmentation_type == \"rotate\":\n",
    "            angle = 3 if is_odd else -3  # Adjusted rotation angles\n",
    "            M = cv2.getRotationMatrix2D((width // 2, height // 2), angle, 1)\n",
    "            frame = cv2.warpAffine(frame, M, (width, height))\n",
    "        elif augmentation_type == \"brightness\":\n",
    "            alpha = 1.05 if is_odd else 0.95  # Adjusted brightness\n",
    "            frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=0)\n",
    "        elif augmentation_type == \"noise\":\n",
    "            noise = np.random.normal(0, 15, frame.shape).astype(np.uint8)  # Add small random noise\n",
    "            frame = cv2.add(frame, noise)\n",
    "\n",
    "        # Write the augmented frame\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Augment videos and create a new CSV file\n",
    "augmented_data = []\n",
    "for idx, row in labels_df.iterrows():\n",
    "    video_path = row['video']\n",
    "    label = row['label']\n",
    "    base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    # Original video path\n",
    "    input_video_path = os.path.join(input_folder, os.path.basename(video_path))\n",
    "    is_odd = (idx % 2 == 1)\n",
    "\n",
    "    # Add original video to the dataset\n",
    "    original_video_path = os.path.join(output_folder, f\"{base_name}_original.mp4\")\n",
    "    if not os.path.exists(original_video_path):\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        if cap.isOpened():\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(original_video_path, fourcc, fps, (width, height))\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                out.write(frame)\n",
    "            cap.release()\n",
    "            out.release()\n",
    "    augmented_data.append({\n",
    "        \"video\": original_video_path,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Apply augmentations\n",
    "    for aug_type in [\"mirrored\", \"rotate\", \"brightness\", \"noise\"]:\n",
    "        output_video_name = f\"{base_name}_{aug_type}.mp4\"\n",
    "        output_video_path = os.path.join(output_folder, output_video_name)\n",
    "        augment_video(input_video_path, output_video_path, aug_type, is_odd)\n",
    "\n",
    "        # Append to dataset\n",
    "        augmented_data.append({\n",
    "            \"video\": output_video_path,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame for the augmented dataset\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Save the new dataset labels to a CSV file\n",
    "augmented_df.to_csv(\"stage4-dataset.csv\", index=False)\n",
    "print(\"Augmented videos saved to stage4-dataset and labels saved to stage4-dataset.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoints extracted and saved to: stage4-dataset-json\\user10_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user10_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user10_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user10_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user10_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user12_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user12_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user12_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user12_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user12_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user13_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user13_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user13_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user13_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user13_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user19_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user19_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user19_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user19_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user19_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user1_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user1_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user1_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user1_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user1_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user20_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user20_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user20_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user20_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user20_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user22_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user22_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user22_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user22_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user22_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user23_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user23_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user23_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user23_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user23_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user2_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user2_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user2_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user2_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user2_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user3_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user3_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user3_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user3_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user3_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user5_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user5_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user5_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user5_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user5_rotate_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user8_brightness_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user8_mirrored_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user8_noise_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user8_original_keypoints.json\n",
      "Keypoints extracted and saved to: stage4-dataset-json\\user8_rotate_keypoints.json\n",
      "All videos processed. Keypoints saved to JSON.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "input_videos_path = \"stage4-dataset\"  # Path to input videos\n",
    "output_json_path = \"stage4-dataset-json\"  # Path to save JSON files\n",
    "os.makedirs(output_json_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def extract_keypoints(video_path, output_json_path, frame_skip=2):\n",
    "    \"\"\"\n",
    "    Extracts x and y coordinates of key landmarks and saves them in a JSON file, skipping frames.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_json_path (str): Path to save extracted keypoints.\n",
    "        frame_skip (int): Process 1 out of 'frame_skip' frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = 0\n",
    "\n",
    "    keypoints_list = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "        # Skip every second frame\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Extract x, y values for selected keypoints (normalized values 0-1)\n",
    "            nose_x, nose_y = landmarks[mp_pose.PoseLandmark.NOSE].x, landmarks[mp_pose.PoseLandmark.NOSE].y\n",
    "            left_shoulder_x, left_shoulder_y = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y\n",
    "            right_shoulder_x, right_shoulder_y = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y\n",
    "            left_hip_x, left_hip_y = landmarks[mp_pose.PoseLandmark.LEFT_HIP].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP].y\n",
    "            right_hip_x, right_hip_y = landmarks[mp_pose.PoseLandmark.RIGHT_HIP].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y\n",
    "\n",
    "            # Store extracted keypoints in normalized values (0 to 1)\n",
    "            keypoints_list.append({\n",
    "                \"frame\": int(cap.get(cv2.CAP_PROP_POS_FRAMES)),\n",
    "                \"nose\": [nose_x, nose_y],\n",
    "                \"left_shoulder\": [left_shoulder_x, left_shoulder_y],\n",
    "                \"right_shoulder\": [right_shoulder_x, right_shoulder_y],\n",
    "                \"left_hip\": [left_hip_x, left_hip_y],\n",
    "                \"right_hip\": [right_hip_x, right_hip_y],\n",
    "            })\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save extracted keypoints to a JSON file\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(keypoints_list, f, indent=4)\n",
    "\n",
    "    print(f\"Keypoints extracted and saved to: {output_json_path}\")\n",
    "\n",
    "# Process all videos in the input folder\n",
    "for video_file in os.listdir(input_videos_path):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(input_videos_path, video_file)\n",
    "        json_filename = os.path.splitext(video_file)[0] + \"_keypoints.json\"\n",
    "        json_output_path = os.path.join(output_json_path, json_filename)\n",
    "\n",
    "        extract_keypoints(video_path, json_output_path, frame_skip=2)\n",
    "\n",
    "print(\"All videos processed. Keypoints saved to JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared:\n",
      "Training set shape: (40, 200, 5), Validation set shape: (18, 200, 5)\n",
      "Number of features per frame: 5\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "json_folder = \"stage4-dataset-json\"  # Folder containing JSON files with extracted features\n",
    "labels_file = \"stage4-dataset.csv\"  # CSV file with video labels (e.g., user1.mp4: 0, user2.mp4: 1)\n",
    "\n",
    "# Load labels (assumes a CSV file with 'video' and 'label' columns)\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "label_mapping = dict(zip(labels_df['video'].str.replace(\".mp4\", \"\"), labels_df['label']))\n",
    "\n",
    "# Function to calculate Euclidean distance between two points\n",
    "def calculate_distance(p1, p2):\n",
    "    return np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "# Function to calculate the angle formed by three points (b is the vertex)\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Dataset preparation\n",
    "X = []  # Features\n",
    "y = []  # Labels\n",
    "max_sequence_length = 200  # Fixed sequence length for LSTM or Transformer\n",
    "\n",
    "for json_file in sorted(os.listdir(json_folder)):\n",
    "    user_id = os.path.splitext(json_file)[0]  # Extract user ID from file name without .json\n",
    "    if user_id not in label_mapping:\n",
    "        print(f\"Label not found for {user_id}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    label = label_mapping[user_id]  # Get label for the user\n",
    "    json_path = os.path.join(json_folder, json_file)\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract meaningful features\n",
    "    user_features = []\n",
    "    for frame in data:\n",
    "        try:\n",
    "            # Extract x and y values for selected keypoints\n",
    "            nose = frame[\"nose\"]\n",
    "            left_shoulder = frame[\"left_shoulder\"]\n",
    "            right_shoulder = frame[\"right_shoulder\"]\n",
    "            left_hip = frame[\"left_hip\"]\n",
    "            right_hip = frame[\"right_hip\"]\n",
    "\n",
    "            # Compute meaningful features\n",
    "            shoulder_width = calculate_distance(left_shoulder, right_shoulder)\n",
    "            hip_width = calculate_distance(left_hip, right_hip)\n",
    "            torso_length = calculate_distance(left_shoulder, left_hip)\n",
    "            shoulder_hip_angle = calculate_angle(left_shoulder, nose, left_hip)\n",
    "            hip_movement = right_hip[0] - left_hip[0]  # Track x movement\n",
    "\n",
    "            # Combine features into a single vector\n",
    "            features = [\n",
    "                shoulder_width,  # Shoulder width\n",
    "                hip_width,       # Hip width\n",
    "                torso_length,    # Torso length\n",
    "                shoulder_hip_angle,  # Angle from shoulder to hip\n",
    "                hip_movement     # X-axis movement tracking\n",
    "            ]\n",
    "            user_features.append(features)\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key {e} in {json_file}. Skipping frame...\")\n",
    "            continue\n",
    "\n",
    "    # Append features and label if data exists\n",
    "    if user_features:\n",
    "        X.append(user_features)\n",
    "        y.append(label)\n",
    "\n",
    "# Pad sequences to ensure consistent lengths\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# Model Constants\n",
    "sequence_length = X_train.shape[1]  # Fixed length of sequences\n",
    "num_features = X_train.shape[2]  # Number of features per frame\n",
    "num_classes = len(np.unique(y_train))  # Number of unique labels\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}\")\n",
    "print(f\"Number of features per frame: {num_features}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.1764705882352942, 1: 0.8695652173913043}\n",
      "Epoch 1/60\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.7023 - accuracy: 0.5250 - val_loss: 0.6911 - val_accuracy: 0.5556\n",
      "Epoch 2/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.7232 - accuracy: 0.3750 - val_loss: 0.6933 - val_accuracy: 0.4444\n",
      "Epoch 3/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.4444\n",
      "Epoch 4/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.6918 - accuracy: 0.5750 - val_loss: 0.6885 - val_accuracy: 0.5556\n",
      "Epoch 5/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.7127 - accuracy: 0.5500 - val_loss: 0.6900 - val_accuracy: 0.5556\n",
      "Epoch 6/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6960 - accuracy: 0.5500 - val_loss: 0.6914 - val_accuracy: 0.5556\n",
      "Epoch 7/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6921 - accuracy: 0.5000 - val_loss: 0.6957 - val_accuracy: 0.4444\n",
      "Epoch 8/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6976 - accuracy: 0.4750 - val_loss: 0.6964 - val_accuracy: 0.4444\n",
      "Epoch 9/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7039 - accuracy: 0.3500 - val_loss: 0.6927 - val_accuracy: 0.5556\n",
      "Epoch 10/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6941 - accuracy: 0.4750 - val_loss: 0.6933 - val_accuracy: 0.4444\n",
      "Epoch 11/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6917 - accuracy: 0.5250 - val_loss: 0.6955 - val_accuracy: 0.4444\n",
      "Epoch 12/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6999 - accuracy: 0.4500 - val_loss: 0.6982 - val_accuracy: 0.4444\n",
      "Epoch 13/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6951 - accuracy: 0.4500 - val_loss: 0.6935 - val_accuracy: 0.4444\n",
      "Epoch 14/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6936 - accuracy: 0.4750 - val_loss: 0.6905 - val_accuracy: 0.5556\n",
      "Epoch 15/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7083 - accuracy: 0.5500 - val_loss: 0.7033 - val_accuracy: 0.4444\n",
      "Epoch 16/60\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6938 - accuracy: 0.4250 - val_loss: 0.7052 - val_accuracy: 0.4444\n",
      "Epoch 17/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7017 - accuracy: 0.4250 - val_loss: 0.7141 - val_accuracy: 0.4444\n",
      "Epoch 18/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.8001 - accuracy: 0.5000 - val_loss: 0.7001 - val_accuracy: 0.5556\n",
      "Epoch 19/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7105 - accuracy: 0.5250 - val_loss: 0.6833 - val_accuracy: 0.5556\n",
      "Epoch 20/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7035 - accuracy: 0.5000 - val_loss: 0.7028 - val_accuracy: 0.4444\n",
      "Epoch 21/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7439 - accuracy: 0.4000 - val_loss: 0.6921 - val_accuracy: 0.5556\n",
      "Epoch 22/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6967 - accuracy: 0.5000 - val_loss: 0.6943 - val_accuracy: 0.4444\n",
      "Epoch 23/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6893 - accuracy: 0.5000 - val_loss: 0.6969 - val_accuracy: 0.4444\n",
      "Epoch 24/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6946 - accuracy: 0.4250 - val_loss: 0.6911 - val_accuracy: 0.6111\n",
      "Epoch 25/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7044 - accuracy: 0.4500 - val_loss: 0.6776 - val_accuracy: 0.5556\n",
      "Epoch 26/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6922 - accuracy: 0.5750 - val_loss: 0.6842 - val_accuracy: 0.5556\n",
      "Epoch 27/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6944 - accuracy: 0.6000 - val_loss: 0.6798 - val_accuracy: 0.5556\n",
      "Epoch 28/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6992 - accuracy: 0.5000 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 29/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6835 - val_accuracy: 0.6667\n",
      "Epoch 30/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6929 - accuracy: 0.5000 - val_loss: 0.6811 - val_accuracy: 0.5556\n",
      "Epoch 31/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6977 - accuracy: 0.4750 - val_loss: 0.7008 - val_accuracy: 0.4444\n",
      "Epoch 32/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6982 - accuracy: 0.4750 - val_loss: 0.6643 - val_accuracy: 0.6667\n",
      "Epoch 33/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7297 - accuracy: 0.3500 - val_loss: 0.7292 - val_accuracy: 0.3889\n",
      "Epoch 34/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.8060 - val_accuracy: 0.3889\n",
      "Epoch 35/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7348 - accuracy: 0.6000 - val_loss: 0.7979 - val_accuracy: 0.3889\n",
      "Epoch 36/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6606 - accuracy: 0.6000 - val_loss: 0.8728 - val_accuracy: 0.3333\n",
      "Epoch 37/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6475 - accuracy: 0.6000 - val_loss: 0.8176 - val_accuracy: 0.3333\n",
      "Epoch 38/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6758 - accuracy: 0.5750 - val_loss: 0.8001 - val_accuracy: 0.3333\n",
      "Epoch 39/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6723 - accuracy: 0.6500 - val_loss: 0.8016 - val_accuracy: 0.3889\n",
      "Epoch 40/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6796 - accuracy: 0.6250 - val_loss: 0.8599 - val_accuracy: 0.3889\n",
      "Epoch 41/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6535 - accuracy: 0.6750 - val_loss: 0.8438 - val_accuracy: 0.3889\n",
      "Epoch 42/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6974 - accuracy: 0.5750 - val_loss: 0.7344 - val_accuracy: 0.4444\n",
      "Epoch 43/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7219 - accuracy: 0.4500 - val_loss: 0.7162 - val_accuracy: 0.3889\n",
      "Epoch 44/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7193 - accuracy: 0.5000 - val_loss: 0.7609 - val_accuracy: 0.3889\n",
      "Epoch 45/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7012 - accuracy: 0.6000 - val_loss: 0.8034 - val_accuracy: 0.3889\n",
      "Epoch 46/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6494 - accuracy: 0.6500 - val_loss: 0.8983 - val_accuracy: 0.3889\n",
      "Epoch 47/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7189 - accuracy: 0.6500 - val_loss: 0.8727 - val_accuracy: 0.3889\n",
      "Epoch 48/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6144 - accuracy: 0.6500 - val_loss: 0.8607 - val_accuracy: 0.3889\n",
      "Epoch 49/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6829 - accuracy: 0.6500 - val_loss: 0.8314 - val_accuracy: 0.3889\n",
      "Epoch 50/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6714 - accuracy: 0.6250 - val_loss: 0.8039 - val_accuracy: 0.3889\n",
      "Epoch 51/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6570 - accuracy: 0.6500 - val_loss: 0.7917 - val_accuracy: 0.3889\n",
      "Epoch 52/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6569 - accuracy: 0.6500 - val_loss: 0.7883 - val_accuracy: 0.3889\n",
      "Epoch 53/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6440 - accuracy: 0.6500 - val_loss: 0.7999 - val_accuracy: 0.3889\n",
      "Epoch 54/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6748 - accuracy: 0.6500 - val_loss: 0.7984 - val_accuracy: 0.3889\n",
      "Epoch 55/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6378 - accuracy: 0.6500 - val_loss: 0.8041 - val_accuracy: 0.3889\n",
      "Epoch 56/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6834 - accuracy: 0.6500 - val_loss: 0.8010 - val_accuracy: 0.3889\n",
      "Epoch 57/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6593 - accuracy: 0.6500 - val_loss: 0.7741 - val_accuracy: 0.3889\n",
      "Epoch 58/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6534 - accuracy: 0.6000 - val_loss: 0.7636 - val_accuracy: 0.3889\n",
      "Epoch 59/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6672 - accuracy: 0.6250 - val_loss: 0.7707 - val_accuracy: 0.3889\n",
      "Epoch 60/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6261 - accuracy: 0.6500 - val_loss: 0.7915 - val_accuracy: 0.3889\n",
      "Model training complete and saved as 'stage3-spear.keras'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights to handle data imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='tanh', return_sequences=True, input_shape=(sequence_length, num_features)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='tanh', return_sequences=False),  # Add L2 regularization\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),  # Add L2 regularization to dense layer\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Output: Value between 0 and 1\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01), \n",
    "    loss='binary_crossentropy',  # Using binary crossentropy for binary classification\n",
    "    metrics=['accuracy']  # Train on accuracy metric\n",
    ")\n",
    "\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=60,  # Adjust based on dataset size\n",
    "    batch_size=16,  # Adjust based on memory constraints\n",
    "    class_weight=class_weights_dict,  # Use class weights to address imbalance\n",
    "   #  callbacks=[early_stopping],  # Early stopping to prevent overfitting\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"stage4-final.keras\")\n",
    "print(\"Model training complete and saved as 'stage3-spear.keras'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
