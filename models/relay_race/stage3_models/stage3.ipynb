{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated video saved to: stage3-interpolated\\user1\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user1\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user10\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user10\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user12\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user12\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user13\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user13\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user19\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user19\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user2\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user2\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user20\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user20\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user22\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user22\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user23\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user23\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user3\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user3\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user5\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user5\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user6\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user6\\e.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user8\\a.mp4\n",
      "Interpolated video saved to: stage3-interpolated\\user8\\e.mp4\n",
      "Interpolation complete for all videos.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_folder = \"stage3\"\n",
    "output_folder = \"stage3-interpolated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def interpolate_frames(frame1, frame2, num_interpolations=2):\n",
    "    \"\"\"\n",
    "    Interpolates between two frames using linear interpolation.\n",
    "    \n",
    "    Args:\n",
    "        frame1 (numpy.ndarray): First frame.\n",
    "        frame2 (numpy.ndarray): Second frame.\n",
    "        num_interpolations (int): Number of interpolated frames to generate.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of interpolated frames.\n",
    "    \"\"\"\n",
    "    interpolated_frames = []\n",
    "    for i in range(1, num_interpolations + 1):\n",
    "        alpha = i / (num_interpolations + 1)  # Interpolation ratio\n",
    "        interpolated_frame = cv2.addWeighted(frame1, 1 - alpha, frame2, alpha, 0)\n",
    "        interpolated_frames.append(interpolated_frame)\n",
    "    return interpolated_frames\n",
    "\n",
    "def process_video(video_path, output_path, num_interpolations=2):\n",
    "    \"\"\"\n",
    "    Processes a video to add interpolated frames and saves the result.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the interpolated video.\n",
    "        num_interpolations (int): Number of interpolated frames between each pair of original frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "    \n",
    "    # New FPS after interpolation\n",
    "    new_fps = fps * (num_interpolations + 1)\n",
    "    out = cv2.VideoWriter(output_path, fourcc, new_fps, (width, height))\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    while ret:\n",
    "        ret, next_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Write the original frame\n",
    "        out.write(prev_frame)\n",
    "\n",
    "        # Generate and write interpolated frames\n",
    "        interpolated_frames = interpolate_frames(prev_frame, next_frame, num_interpolations)\n",
    "        for frame in interpolated_frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        # Update the previous frame\n",
    "        prev_frame = next_frame\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Interpolated video saved to: {output_path}\")\n",
    "\n",
    "# Process videos inside each user's folder\n",
    "num_interpolations = 2  # Number of interpolated frames between each pair\n",
    "\n",
    "for user_folder in os.listdir(input_folder):\n",
    "    user_folder_path = os.path.join(input_folder, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        user_output_folder = os.path.join(output_folder, user_folder)\n",
    "        os.makedirs(user_output_folder, exist_ok=True)\n",
    "\n",
    "        for video_file in [\"a.mp4\", \"e.mp4\"]:\n",
    "            input_path = os.path.join(user_folder_path, video_file)\n",
    "            output_path = os.path.join(user_output_folder, video_file)\n",
    "            \n",
    "            if os.path.exists(input_path):\n",
    "                process_video(input_path, output_path, num_interpolations=num_interpolations)\n",
    "            else:\n",
    "                print(f\"Video {video_file} not found in {user_folder}\")\n",
    "\n",
    "print(\"Interpolation complete for all videos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for e.mp4:\n",
      "  Average frames: 27.00\n",
      "  Minimum frames: 27\n",
      "  Maximum frames: 27\n",
      "Statistics for a.mp4:\n",
      "  Average frames: 132.23\n",
      "  Minimum frames: 69\n",
      "  Maximum frames: 234\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the videos\n",
    "stage_path = \"stage3-interpolated\"\n",
    "\n",
    "# Dictionary to store frame counts for 1.mp4 and 2.mp4 separately\n",
    "frame_counts = {\"e.mp4\": [], \"a.mp4\": []}\n",
    "\n",
    "# Iterate through user folders\n",
    "for user_folder in os.listdir(stage_path):\n",
    "    user_folder_path = os.path.join(stage_path, user_folder)\n",
    "    \n",
    "    if os.path.isdir(user_folder_path):\n",
    "        for video_file in [\"e.mp4\", \"a.mp4\"]:\n",
    "            video_path = os.path.join(user_folder_path, video_file)\n",
    "\n",
    "            # Check if video exists in the user's folder\n",
    "            if os.path.exists(video_path):\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                \n",
    "                if not cap.isOpened():\n",
    "                    print(f\"Error opening video file: {video_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Get the total number of frames\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                frame_counts[video_file].append(total_frames)\n",
    "                cap.release()\n",
    "            else:\n",
    "                print(f\"Video {video_file} not found in {user_folder_path}\")\n",
    "\n",
    "# Calculate statistics for 1.mp4 and 2.mp4\n",
    "for video_file in frame_counts:\n",
    "    if frame_counts[video_file]:\n",
    "        avg_frames = sum(frame_counts[video_file]) / len(frame_counts[video_file])\n",
    "        min_frames = min(frame_counts[video_file])\n",
    "        max_frames = max(frame_counts[video_file])\n",
    "\n",
    "        print(f\"Statistics for {video_file}:\")\n",
    "        print(f\"  Average frames: {avg_frames:.2f}\")\n",
    "        print(f\"  Minimum frames: {min_frames}\")\n",
    "        print(f\"  Maximum frames: {max_frames}\")\n",
    "    else:\n",
    "        print(f\"No valid videos found for {video_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed a.mp4 in user1 to 130 frames\n",
      "Processed e.mp4 in user1 to 27 frames\n",
      "Processed a.mp4 in user10 to 130 frames\n",
      "Processed e.mp4 in user10 to 27 frames\n",
      "Processed a.mp4 in user12 to 130 frames\n",
      "Processed e.mp4 in user12 to 27 frames\n",
      "Processed a.mp4 in user13 to 130 frames\n",
      "Processed e.mp4 in user13 to 27 frames\n",
      "Processed a.mp4 in user19 to 130 frames\n",
      "Processed e.mp4 in user19 to 27 frames\n",
      "Processed a.mp4 in user2 to 130 frames\n",
      "Processed e.mp4 in user2 to 27 frames\n",
      "Processed a.mp4 in user20 to 130 frames\n",
      "Processed e.mp4 in user20 to 27 frames\n",
      "Processed a.mp4 in user22 to 130 frames\n",
      "Processed e.mp4 in user22 to 27 frames\n",
      "Processed a.mp4 in user23 to 130 frames\n",
      "Processed e.mp4 in user23 to 27 frames\n",
      "Processed a.mp4 in user3 to 130 frames\n",
      "Processed e.mp4 in user3 to 27 frames\n",
      "Processed a.mp4 in user5 to 130 frames\n",
      "Processed e.mp4 in user5 to 27 frames\n",
      "Processed a.mp4 in user6 to 130 frames\n",
      "Processed e.mp4 in user6 to 27 frames\n",
      "Processed a.mp4 in user8 to 130 frames\n",
      "Processed e.mp4 in user8 to 27 frames\n",
      "All videos processed and standardized.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to input and output directories\n",
    "input_dir = \"stage3-interpolated\"\n",
    "output_dir = \"stage3-processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Target number of frames for each video\n",
    "target_frames_dict = {\n",
    "    \"a.mp4\": 130,\n",
    "    \"e.mp4\": 27\n",
    "}\n",
    "\n",
    "def truncate_or_pad_video(video_path, output_path, target_frames):\n",
    "    \"\"\"\n",
    "    Truncate or pad a video to ensure it has exactly `target_frames` frames.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the processed video.\n",
    "        target_frames (int): Desired number of frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create VideoWriter for the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < target_frames:\n",
    "        # Pad by duplicating the last frame\n",
    "        while len(frames) < target_frames:\n",
    "            frames.append(frames[-1])\n",
    "    elif len(frames) > target_frames:\n",
    "        # Truncate to the required number of frames\n",
    "        frames = frames[:target_frames]\n",
    "\n",
    "    # Write the processed frames to the output video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "# Process all videos in each user's folder\n",
    "for user_folder in os.listdir(input_dir):\n",
    "    user_input_folder = os.path.join(input_dir, user_folder)\n",
    "    user_output_folder = os.path.join(output_dir, user_folder)\n",
    "    os.makedirs(user_output_folder, exist_ok=True)\n",
    "\n",
    "    if os.path.isdir(user_input_folder):\n",
    "        for video_file, target_frames in target_frames_dict.items():\n",
    "            input_path = os.path.join(user_input_folder, video_file)\n",
    "            output_path = os.path.join(user_output_folder, video_file)\n",
    "\n",
    "            if os.path.exists(input_path):\n",
    "                truncate_or_pad_video(input_path, output_path, target_frames)\n",
    "                print(f\"Processed {video_file} in {user_folder} to {target_frames} frames\")\n",
    "            else:\n",
    "                print(f\"Video {video_file} not found in {user_folder}\")\n",
    "\n",
    "print(\"All videos processed and standardized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented videos saved to stage3-dataset and labels saved to stage3-dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_folder = \"stage3-processed\"\n",
    "output_folder = \"stage3-dataset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"stage3.csv\")\n",
    "\n",
    "# Augmentation Functions\n",
    "def augment_video(video_path, output_path, augmentation_type, is_odd):\n",
    "    \"\"\"\n",
    "    Augment the video with the specified augmentation type and save it.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the augmented video.\n",
    "        augmentation_type (str): Type of augmentation ('rotate', 'brightness', 'noise', etc.).\n",
    "        is_odd (bool): Whether the video index is odd or even.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "\n",
    "    # Create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply augmentation\n",
    "        if augmentation_type == \"mirrored\":\n",
    "            frame = cv2.flip(frame, 1)  # Horizontal flip\n",
    "        elif augmentation_type == \"rotate\":\n",
    "            angle = 3 if is_odd else -3  # Adjusted rotation angles\n",
    "            M = cv2.getRotationMatrix2D((width // 2, height // 2), angle, 1)\n",
    "            frame = cv2.warpAffine(frame, M, (width, height))\n",
    "        elif augmentation_type == \"brightness\":\n",
    "            alpha = 1.05 if is_odd else 0.95  # Adjusted brightness\n",
    "            frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=0)\n",
    "        elif augmentation_type == \"noise\":\n",
    "            noise = np.random.normal(0, 15, frame.shape).astype(np.uint8)  # Add small random noise\n",
    "            frame = cv2.add(frame, noise)\n",
    "\n",
    "        # Write the augmented frame\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "def combine_videos(video1_path, video2_path, output_path):\n",
    "    \"\"\"\n",
    "    Combines two videos by concatenating their frames.\n",
    "    \n",
    "    Args:\n",
    "        video1_path (str): Path to the first video (e.mp4).\n",
    "        video2_path (str): Path to the second video (a.mp4).\n",
    "        output_path (str): Path to save the combined video.\n",
    "    \"\"\"\n",
    "    cap1 = cv2.VideoCapture(video1_path)\n",
    "    cap2 = cv2.VideoCapture(video2_path)\n",
    "\n",
    "    if not cap1.isOpened() or not cap2.isOpened():\n",
    "        print(f\"Error: Unable to open one of the videos: {video1_path} or {video2_path}\")\n",
    "        return\n",
    "\n",
    "    width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap1.isOpened():\n",
    "        ret, frame = cap1.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "\n",
    "    while cap2.isOpened():\n",
    "        ret, frame = cap2.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    out.release()\n",
    "\n",
    "# Augment videos and create a new CSV file\n",
    "augmented_data = []\n",
    "for idx, row in labels_df.iterrows():\n",
    "    user_folder = row['video']\n",
    "    label = row['label']\n",
    "\n",
    "    video_e_path = os.path.join(input_folder, user_folder, \"e.mp4\")\n",
    "    video_a_path = os.path.join(input_folder, user_folder, \"a.mp4\")\n",
    "    \n",
    "    if not os.path.exists(video_e_path) or not os.path.exists(video_a_path):\n",
    "        print(f\"Skipping user {user_folder} due to missing videos.\")\n",
    "        continue\n",
    "\n",
    "    combined_video_path = os.path.join(output_folder, f\"{user_folder}_original.mp4\")\n",
    "    combine_videos(video_e_path, video_a_path, combined_video_path)\n",
    "\n",
    "    augmented_data.append({\n",
    "        \"video\": combined_video_path,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Apply augmentations\n",
    "    is_odd = (idx % 2 == 1)\n",
    "    for aug_type in [\"mirrored\", \"rotate\", \"brightness\", \"noise\"]:\n",
    "        output_video_name = f\"{user_folder}_{aug_type}.mp4\"\n",
    "        output_video_path = os.path.join(output_folder, output_video_name)\n",
    "        augment_video(combined_video_path, output_video_path, aug_type, is_odd)\n",
    "\n",
    "        # Append augmented video to dataset\n",
    "        augmented_data.append({\n",
    "            \"video\": output_video_path,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame for the augmented dataset\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Save the new dataset labels to a CSV file\n",
    "augmented_df.to_csv(\"stage3-dataset.csv\", index=False)\n",
    "print(\"Augmented videos saved to stage3-dataset and labels saved to stage3-dataset.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: user10_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user10_brightness_keypoints.json\n",
      "Processed: user10_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user10_mirrored_keypoints.json\n",
      "Processed: user10_noise.mp4 - Keypoints saved to stage3-dataset-json\\user10_noise_keypoints.json\n",
      "Processed: user10_original.mp4 - Keypoints saved to stage3-dataset-json\\user10_original_keypoints.json\n",
      "Processed: user10_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user10_rotate_keypoints.json\n",
      "Processed: user12_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user12_brightness_keypoints.json\n",
      "Processed: user12_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user12_mirrored_keypoints.json\n",
      "Processed: user12_noise.mp4 - Keypoints saved to stage3-dataset-json\\user12_noise_keypoints.json\n",
      "Processed: user12_original.mp4 - Keypoints saved to stage3-dataset-json\\user12_original_keypoints.json\n",
      "Processed: user12_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user12_rotate_keypoints.json\n",
      "Processed: user13_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user13_brightness_keypoints.json\n",
      "Processed: user13_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user13_mirrored_keypoints.json\n",
      "Processed: user13_noise.mp4 - Keypoints saved to stage3-dataset-json\\user13_noise_keypoints.json\n",
      "Processed: user13_original.mp4 - Keypoints saved to stage3-dataset-json\\user13_original_keypoints.json\n",
      "Processed: user13_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user13_rotate_keypoints.json\n",
      "Processed: user19_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user19_brightness_keypoints.json\n",
      "Processed: user19_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user19_mirrored_keypoints.json\n",
      "Processed: user19_noise.mp4 - Keypoints saved to stage3-dataset-json\\user19_noise_keypoints.json\n",
      "Processed: user19_original.mp4 - Keypoints saved to stage3-dataset-json\\user19_original_keypoints.json\n",
      "Processed: user19_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user19_rotate_keypoints.json\n",
      "Processed: user1_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user1_brightness_keypoints.json\n",
      "Processed: user1_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user1_mirrored_keypoints.json\n",
      "Processed: user1_noise.mp4 - Keypoints saved to stage3-dataset-json\\user1_noise_keypoints.json\n",
      "Processed: user1_original.mp4 - Keypoints saved to stage3-dataset-json\\user1_original_keypoints.json\n",
      "Processed: user1_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user1_rotate_keypoints.json\n",
      "Processed: user20_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user20_brightness_keypoints.json\n",
      "Processed: user20_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user20_mirrored_keypoints.json\n",
      "Processed: user20_noise.mp4 - Keypoints saved to stage3-dataset-json\\user20_noise_keypoints.json\n",
      "Processed: user20_original.mp4 - Keypoints saved to stage3-dataset-json\\user20_original_keypoints.json\n",
      "Processed: user20_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user20_rotate_keypoints.json\n",
      "Processed: user22_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user22_brightness_keypoints.json\n",
      "Processed: user22_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user22_mirrored_keypoints.json\n",
      "Processed: user22_noise.mp4 - Keypoints saved to stage3-dataset-json\\user22_noise_keypoints.json\n",
      "Processed: user22_original.mp4 - Keypoints saved to stage3-dataset-json\\user22_original_keypoints.json\n",
      "Processed: user22_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user22_rotate_keypoints.json\n",
      "Processed: user23_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user23_brightness_keypoints.json\n",
      "Processed: user23_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user23_mirrored_keypoints.json\n",
      "Processed: user23_noise.mp4 - Keypoints saved to stage3-dataset-json\\user23_noise_keypoints.json\n",
      "Processed: user23_original.mp4 - Keypoints saved to stage3-dataset-json\\user23_original_keypoints.json\n",
      "Processed: user23_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user23_rotate_keypoints.json\n",
      "Processed: user2_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user2_brightness_keypoints.json\n",
      "Processed: user2_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user2_mirrored_keypoints.json\n",
      "Processed: user2_noise.mp4 - Keypoints saved to stage3-dataset-json\\user2_noise_keypoints.json\n",
      "Processed: user2_original.mp4 - Keypoints saved to stage3-dataset-json\\user2_original_keypoints.json\n",
      "Processed: user2_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user2_rotate_keypoints.json\n",
      "Processed: user3_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user3_brightness_keypoints.json\n",
      "Processed: user3_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user3_mirrored_keypoints.json\n",
      "Processed: user3_noise.mp4 - Keypoints saved to stage3-dataset-json\\user3_noise_keypoints.json\n",
      "Processed: user3_original.mp4 - Keypoints saved to stage3-dataset-json\\user3_original_keypoints.json\n",
      "Processed: user3_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user3_rotate_keypoints.json\n",
      "Processed: user5_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user5_brightness_keypoints.json\n",
      "Processed: user5_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user5_mirrored_keypoints.json\n",
      "Processed: user5_noise.mp4 - Keypoints saved to stage3-dataset-json\\user5_noise_keypoints.json\n",
      "Processed: user5_original.mp4 - Keypoints saved to stage3-dataset-json\\user5_original_keypoints.json\n",
      "Processed: user5_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user5_rotate_keypoints.json\n",
      "Processed: user6_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user6_brightness_keypoints.json\n",
      "Processed: user6_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user6_mirrored_keypoints.json\n",
      "Processed: user6_noise.mp4 - Keypoints saved to stage3-dataset-json\\user6_noise_keypoints.json\n",
      "Processed: user6_original.mp4 - Keypoints saved to stage3-dataset-json\\user6_original_keypoints.json\n",
      "Processed: user6_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user6_rotate_keypoints.json\n",
      "Processed: user8_brightness.mp4 - Keypoints saved to stage3-dataset-json\\user8_brightness_keypoints.json\n",
      "Processed: user8_mirrored.mp4 - Keypoints saved to stage3-dataset-json\\user8_mirrored_keypoints.json\n",
      "Processed: user8_noise.mp4 - Keypoints saved to stage3-dataset-json\\user8_noise_keypoints.json\n",
      "Processed: user8_original.mp4 - Keypoints saved to stage3-dataset-json\\user8_original_keypoints.json\n",
      "Processed: user8_rotate.mp4 - Keypoints saved to stage3-dataset-json\\user8_rotate_keypoints.json\n",
      "All video processing completed. Keypoints saved as JSON.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate the angle formed by three points (a, b, c) where 'b' is the vertex.\n",
    "    \"\"\"\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba, bc = a - b, c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Paths\n",
    "input_videos_path = \"stage3-dataset\"  # Path to input videos\n",
    "output_json_path = \"stage3-dataset-json\"  # Path to save JSON files\n",
    "os.makedirs(output_json_path, exist_ok=True)\n",
    "\n",
    "# Process all videos in the input directory\n",
    "for file in os.listdir(input_videos_path):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        video_file_path = os.path.join(input_videos_path, file)\n",
    "        cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "        keypoints_data = []\n",
    "        frame_count = 0\n",
    "        processed_frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Skip every second frame\n",
    "            if frame_count % 2 == 0:\n",
    "                continue\n",
    "\n",
    "            processed_frame_count += 1\n",
    "\n",
    "            # Convert frame to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = pose.process(frame_rgb)\n",
    "\n",
    "            if result.pose_landmarks:\n",
    "                landmarks = result.pose_landmarks.landmark\n",
    "\n",
    "                # Extract keypoints for analysis\n",
    "                nose = [landmarks[mp_pose.PoseLandmark.NOSE].x, landmarks[mp_pose.PoseLandmark.NOSE].y]\n",
    "                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "                                 landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "                                  landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_HIP].y]\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y]\n",
    "\n",
    "                # Calculate body angles and movement\n",
    "                head_angle = calculate_angle(left_shoulder, nose, right_shoulder)\n",
    "                hip_movement = right_hip[0] - left_hip[0]  # Horizontal movement tracking\n",
    "\n",
    "                # Determine whether the frame belongs to baton exchange or not\n",
    "                is_exchange = 1 if processed_frame_count <= 27 else 0\n",
    "\n",
    "                # Store extracted features\n",
    "                keypoints_data.append({\n",
    "                    \"frame\": processed_frame_count,\n",
    "                    \"is_exchange\": is_exchange,\n",
    "                    \"head_angle\": head_angle,\n",
    "                    \"hip_movement\": hip_movement,\n",
    "                    \"nose_x\": nose[0],\n",
    "                    \"nose_y\": nose[1],\n",
    "                    \"left_shoulder_x\": left_shoulder[0],\n",
    "                    \"left_shoulder_y\": left_shoulder[1],\n",
    "                    \"right_shoulder_x\": right_shoulder[0],\n",
    "                    \"right_shoulder_y\": right_shoulder[1],\n",
    "                    \"left_hip_x\": left_hip[0],\n",
    "                    \"left_hip_y\": left_hip[1],\n",
    "                    \"right_hip_x\": right_hip[0],\n",
    "                    \"right_hip_y\": right_hip[1]\n",
    "                })\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Save keypoints to a JSON file\n",
    "        json_filename = os.path.splitext(file)[0] + \"_keypoints.json\"\n",
    "        json_file_path = os.path.join(output_json_path, json_filename)\n",
    "        with open(json_file_path, \"w\") as json_file:\n",
    "            json.dump(keypoints_data, json_file, indent=4)\n",
    "\n",
    "        print(f\"Processed: {file} - Keypoints saved to {json_file_path}\")\n",
    "\n",
    "print(\"All video processing completed. Keypoints saved as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared:\n",
      "Training set shape: (42, 250, 6), Validation set shape: (18, 250, 6)\n",
      "Number of features per frame: 6\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "json_folder = \"stage3-dataset-json\"  # Folder containing JSON files with extracted features\n",
    "labels_file = \"stage3-dataset.csv\"  # CSV file with video labels (e.g., user1_original.mp4: 0)\n",
    "\n",
    "# Load labels (assumes a CSV file with 'video' and 'label' columns)\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "label_mapping = dict(zip(labels_df['video'], labels_df['label']))\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(p1, p2):\n",
    "    return np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "# Function to calculate angles between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba, bc = a - b, c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Dataset preparation\n",
    "X = []  # Features\n",
    "y = []  # Labels\n",
    "max_sequence_length = 250  # Fixed sequence length for LSTM\n",
    "\n",
    "for json_file in sorted(os.listdir(json_folder)):\n",
    "    video_name = os.path.splitext(json_file)[0] + \".mp4\"  # Get full video filename\n",
    "    if video_name not in label_mapping:\n",
    "        print(f\"Label not found for {video_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    label = label_mapping[video_name]  # Get label for the video\n",
    "    json_path = os.path.join(json_folder, json_file)\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract meaningful features\n",
    "    video_features = []\n",
    "    for frame in data:\n",
    "        try:\n",
    "            # Extract coordinates\n",
    "            nose = [frame[\"nose_x\"], frame[\"nose_y\"]]\n",
    "            left_shoulder = [frame[\"left_shoulder_x\"], frame[\"left_shoulder_y\"]]\n",
    "            right_shoulder = [frame[\"right_shoulder_x\"], frame[\"right_shoulder_y\"]]\n",
    "            left_hip = [frame[\"left_hip_x\"], frame[\"left_hip_y\"]]\n",
    "            right_hip = [frame[\"right_hip_x\"], frame[\"right_hip_y\"]]\n",
    "\n",
    "            # Compute meaningful features\n",
    "            shoulder_distance = calculate_distance(left_shoulder, right_shoulder)\n",
    "            hip_distance = calculate_distance(left_hip, right_hip)\n",
    "            torso_length = calculate_distance(left_shoulder, left_hip)\n",
    "\n",
    "            # Shoulder-hip and head tilt angles\n",
    "            shoulder_hip_angle = calculate_angle(left_shoulder, left_hip, right_hip)\n",
    "            head_tilt_angle = calculate_angle(left_shoulder, nose, right_shoulder)\n",
    "\n",
    "            # Store reduced features\n",
    "            features = [\n",
    "                shoulder_distance,  # Horizontal upper body width\n",
    "                hip_distance,       # Horizontal lower body width\n",
    "                torso_length,       # Vertical body height\n",
    "                shoulder_hip_angle, # Posture angle\n",
    "                head_tilt_angle,    # Head tilt angle\n",
    "                frame[\"is_exchange\"]  # Frame label for exchange period\n",
    "            ]\n",
    "            video_features.append(features)\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key {e} in {json_file}. Skipping frame...\")\n",
    "            continue\n",
    "\n",
    "    # Append features and label if data is available\n",
    "    if video_features:\n",
    "        X.append(video_features)\n",
    "        y.append(label)\n",
    "\n",
    "# Pad sequences to ensure consistent lengths\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Model Constants\n",
    "sequence_length = X_train.shape[1]  # Fixed length of sequences (250 frames)\n",
    "num_features = X_train.shape[2]  # Number of reduced features per frame\n",
    "num_classes = len(np.unique(y_train))  # Number of unique labels\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}\")\n",
    "print(f\"Number of features per frame: {num_features}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6774193548387096, 1: 1.9090909090909092}\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6930 - val_accuracy: 0.7778\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6932 - val_accuracy: 0.2222\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.2619 - val_loss: 0.6933 - val_accuracy: 0.2222\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.2619 - val_loss: 0.6933 - val_accuracy: 0.2222\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.2619 - val_loss: 0.6932 - val_accuracy: 0.2222\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.2619 - val_loss: 0.6931 - val_accuracy: 0.7778\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6930 - val_accuracy: 0.7778\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6929 - val_accuracy: 0.7778\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6928 - val_accuracy: 0.7778\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7778\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7778\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7778\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7778\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7778\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6926 - val_accuracy: 0.7778\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6926 - val_accuracy: 0.7778\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6925 - val_accuracy: 0.7778\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6925 - val_accuracy: 0.7778\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6924 - val_accuracy: 0.7778\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6923 - val_accuracy: 0.7778\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6923 - val_accuracy: 0.7778\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6922 - val_accuracy: 0.7778\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6921 - val_accuracy: 0.7778\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6921 - val_accuracy: 0.7778\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6921 - val_accuracy: 0.7778\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6920 - val_accuracy: 0.7778\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6919 - val_accuracy: 0.7778\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6918 - val_accuracy: 0.7778\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6917 - val_accuracy: 0.7778\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6917 - val_accuracy: 0.7778\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6918 - val_accuracy: 0.7778\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6919 - val_accuracy: 0.7778\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6921 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6921 - val_accuracy: 0.7778\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6920 - val_accuracy: 0.7778\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.6931 - accuracy: 0.7381 - val_loss: 0.6919 - val_accuracy: 0.7778\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6918 - val_accuracy: 0.7778\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6918 - val_accuracy: 0.7778\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 6s 4s/step - loss: 0.6932 - accuracy: 0.7381 - val_loss: 0.6917 - val_accuracy: 0.7778\n",
      "Model training complete and saved as 'stage3-final.keras'\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.6917 - accuracy: 0.7778\n",
      "Validation Accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights to handle data imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='tanh', return_sequences=True, input_shape=(sequence_length, num_features)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='tanh', return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Output: Value between 0 and 1 for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    loss='binary_crossentropy',  # Using binary crossentropy for binary classification\n",
    "    metrics=['accuracy']  # Train on accuracy metric\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,  # Adjust based on dataset size\n",
    "    batch_size=32,  # Adjust based on memory constraints\n",
    "    class_weight=class_weights_dict,  # Use class weights to address imbalance\n",
    "    callbacks=[early_stopping],  # Early stopping to prevent overfitting\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"stage3-final.keras\")\n",
    "print(\"Model training complete and saved as 'stage3-final.keras'\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
