{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Video processing and pose estimation\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Deep learning models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# NLP tools\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_features(video_path):\n",
    "    \"\"\"\n",
    "    Extracts pose features from a video using MediaPipe.\n",
    "    Returns: Array of keypoints and custom features.\n",
    "    \"\"\"\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    features = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            # Calculate custom features (e.g., relative knee heights)\n",
    "            rel_knee_height = landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y - landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].y\n",
    "            features.append(rel_knee_height)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea5ff76eef4576a59c719d073588db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f690cf11cd27405bb8561c2f18b9c812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e75dac32f749919dec259c6a8bdfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bbffe1bb394ab9a64b943d00a2713d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eb5e82c58947efab3c9d72bb001f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d8b863a7f74d998a2c9c0e8fbd3694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d24564f5d1d445789398df95b082b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436c90376cb940aaa5891427acaf548b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def interpret_language(video_path, text_instruction):\n",
    "    \"\"\"\n",
    "    Uses CLIP to evaluate the video based on a textual instruction.\n",
    "    \"\"\"\n",
    "    # Process video frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    \n",
    "    # Use CLIP to process frames and text\n",
    "    inputs = clip_processor(text=text_instruction, images=frames, return_tensors=\"pt\", padding=True)\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_text = outputs.logits_per_text  # Text-to-image scores\n",
    "    return logits_per_text.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_features(video_path):\n",
    "    \"\"\"\n",
    "    Extracts spatial and temporal features from the video.\n",
    "    Returns: Array of video features.\n",
    "    \"\"\"\n",
    "    # Placeholder for video feature extraction (e.g., using I3D or Timesformer)\n",
    "    # This can be integrated with a pre-trained model like I3D.\n",
    "    return np.random.rand(1, 1024)  # Example feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_layer(pose_features, language_scores, video_features):\n",
    "    \"\"\"\n",
    "    Combines outputs from all agents and predicts a score.\n",
    "    \"\"\"\n",
    "    # Example: Concatenate all feature vectors\n",
    "    combined_features = np.concatenate([pose_features.mean(axis=0, keepdims=True),\n",
    "                                         language_scores,\n",
    "                                         video_features], axis=1)\n",
    "\n",
    "    # Simple MLP for prediction\n",
    "    inputs = Input(shape=(combined_features.shape[1],))\n",
    "    x = Dense(64, activation='relu')(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model, combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dynamic_changes(features):\n",
    "    \"\"\"\n",
    "    Calculate velocity and acceleration from relative knee height features.\n",
    "    - features: Array of relative knee height values over time.\n",
    "    Returns: Tuple (velocity, acceleration).\n",
    "    \"\"\"\n",
    "    velocities = np.diff(features)  # First derivative (velocity)\n",
    "    accelerations = np.diff(velocities)  # Second derivative (acceleration)\n",
    "    \n",
    "    # Pad arrays to match the original length\n",
    "    velocities = np.pad(velocities, (0, 1), mode='constant', constant_values=0)\n",
    "    accelerations = np.pad(accelerations, (0, 2), mode='constant', constant_values=0)\n",
    "    \n",
    "    return velocities, accelerations\n",
    "\n",
    "def extract_pose_features_with_dynamics(video_path):\n",
    "    \"\"\"\n",
    "    Extract pose features with dynamic changes from a video.\n",
    "    Returns: Combined features (relative knee height, velocity, acceleration).\n",
    "    \"\"\"\n",
    "    relative_knee_heights = extract_pose_features(video_path)\n",
    "    velocities, accelerations = calculate_dynamic_changes(relative_knee_heights)\n",
    "    return np.stack([relative_knee_heights, velocities, accelerations], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "def calculate_pose_ratios(landmarks):\n",
    "    \"\"\"\n",
    "    Calculate pose ratios from keypoints.\n",
    "    - landmarks: List of pose landmarks from MediaPipe.\n",
    "    Returns: The ratio of hip-to-knee to knee-to-ankle distance.\n",
    "    \"\"\"\n",
    "    left_hip = np.array([landmarks[mp_pose.PoseLandmark.LEFT_HIP].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_HIP].y])\n",
    "    left_knee = np.array([landmarks[mp_pose.PoseLandmark.LEFT_KNEE].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y])\n",
    "    left_ankle = np.array([landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y])\n",
    "    \n",
    "    # Calculate distances\n",
    "    hip_knee_dist = np.linalg.norm(left_hip - left_knee)\n",
    "    knee_ankle_dist = np.linalg.norm(left_knee - left_ankle)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if knee_ankle_dist == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return hip_knee_dist / knee_ankle_dist\n",
    "\n",
    "def extract_pose_features_with_ratios(video_path):\n",
    "    \"\"\"\n",
    "    Extract pose features with additional ratios from a video.\n",
    "    Returns: Array of pose ratios over time.\n",
    "    \"\"\"\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    ratios = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            ratio = calculate_pose_ratios(landmarks)\n",
    "            ratios.append(ratio)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyframe_selection(features, threshold=-0.1):\n",
    "    \"\"\"\n",
    "    Select keyframes based on relative knee height.\n",
    "    - features: Array of relative knee height values.\n",
    "    - threshold: Threshold below which keyframes are selected.\n",
    "    Returns: Indices of keyframes.\n",
    "    \"\"\"\n",
    "    return [i for i, value in enumerate(features) if value < threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_language_with_keyframes(video_path, text_instruction, keyframes):\n",
    "    \"\"\"\n",
    "    Use CLIP to analyze selected keyframes based on textual instruction.\n",
    "    - video_path: Path to the video.\n",
    "    - text_instruction: Criterion description to check against the video.\n",
    "    - keyframes: Indices of keyframes to analyze.\n",
    "    Returns: Mean score from CLIP for the selected frames.\n",
    "    \"\"\"\n",
    "    # Process video frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx in keyframes:  # Select only relevant keyframes\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        print(f\"No keyframes selected for {video_path}.\")\n",
    "        return 0.0\n",
    "\n",
    "    # Use CLIP to process frames and text\n",
    "    inputs = clip_processor(text=text_instruction, images=frames, return_tensors=\"pt\", padding=True)\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_text = outputs.logits_per_text  # Text-to-image scores\n",
    "    return logits_per_text.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736770515.072040 9632050 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1736770515.148373 9644138 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736770515.162433 9644142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736770515.179216 9644136 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No keyframes selected for /Users/cezar/Desktop/Team Project/AI/shotput/stage1/videos/1_user2.mp4.\n",
      "CLIP Score for /Users/cezar/Desktop/Team Project/AI/shotput/stage1/videos/1_user2.mp4: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example: Process a video with refined CLIP agent\n",
    "video_path = \"/Users/cezar/Desktop/Team Project/AI/shotput/stage1/videos/1_user2.mp4\"  # Update with an actual video path\n",
    "relative_knee_heights = extract_pose_features(video_path)  # Extract relative knee heights\n",
    "keyframes = keyframe_selection(relative_knee_heights)  # Select keyframes\n",
    "\n",
    "text_instruction = \"Does the athlete initiate the glide phase with their left knee bent and positioned lower than the right knee?\"\n",
    "clip_score = interpret_language_with_keyframes(video_path, text_instruction, keyframes)\n",
    "\n",
    "print(f\"CLIP Score for {video_path}: {clip_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
